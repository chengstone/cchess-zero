{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlphaZeroå®è·µâ€”â€”ä¸­å›½è±¡æ£‹ï¼ˆé™„è®ºæ–‡ç¿»è¯‘ï¼‰\n",
    "AlphaZero Practice-Chinese Chess (with paper translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½œè€…ï¼š[`ç¨‹ä¸–ä¸œ`](http://zhihu.com/people/cheng-shi-dong-47)\n",
    "[`GitHub`](https://github.com/chengstone)  [`Mail`](mailto:69558140@163.com)\n",
    "\n",
    "Translation to English by [`ycechungAI`](https://github.com/ycechungAI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å…³äºAlphaGoå’Œåç»­çš„ç‰ˆæœ¬AlphaGo Zeroç­‰æ–°é—»å¤§å®¶éƒ½è€³ç†Ÿèƒ½è¯¦äº†ï¼Œä»Šå¤©æˆ‘ä»¬ä»è®ºæ–‡çš„åˆ†æï¼Œå¹¶ç»“åˆä»£ç æ¥ä¸€èµ·è®¨è®ºä¸‹AlphaZeroåœ¨ä¸­å›½è±¡æ£‹ä¸Šçš„å®è·µã€‚\n",
    "\n",
    "Everyone is familiar with news about AlphaGo and the subsequent version AlphaGo Zero. Today we will discuss the practice of AlphaZero in Chinese chess from the analysis of the paper and combined with the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å®é™…ä¸Šåœ¨GitHubä¸Šèƒ½å¤Ÿçœ‹åˆ°æœ‰å¾ˆå¤šå…³äºAlphaGoçš„å®è·µé¡¹ç›®ï¼ŒåŒ…æ‹¬[`å›½é™…è±¡æ£‹`](https://github.com/Zeta36/chess-alpha-zero)ã€[`å›´æ£‹`](https://github.com/gcp/leela-zero)ã€[`äº”å­æ£‹`](https://github.com/junxiaosong/AlphaZero_Gomoku)ã€[`é»‘ç™½æ£‹`](https://github.com/mokemokechicken/reversi-alpha-zero)ç­‰ç­‰ï¼Œæˆ‘æœ‰ä¸ªå¥½å‹åœ¨å®è·µéº»å°†ã€‚\n",
    "\n",
    "In fact, you can see many practical projects about AlphaGo on GitHub, including [`Chess`](https://github.com/Zeta36/chess-alpha-zero), [`Go`](https://github.com/gcp/leela-zero), [`Gomoku`](https://github.com/junxiao/AlphaZero_Gomoku), [`Othello`](https://github.com/mokemokechicken/reversi-alpha-zero) Wait, I have a friend who is practicing Mahjong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»ç®—æ³•ä¸Šæ¥è¯´ï¼Œå¤§å®¶éƒ½æ˜¯åŸºäºAlphaGo Zero / AlphaZeroçš„è®ºæ–‡æ¥å®ç°çš„ï¼Œå·®åˆ«åœ¨äºä¸åŒGameçš„è§„åˆ™å’Œä½¿ç”¨äº†ä¸åŒçš„trickã€‚\n",
    "\n",
    "In terms of algorithms, everyone is based on AlphaGo Zero / AlphaZero's papers. The difference lies in the rules of different games and the use of different tricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è®ºæ–‡åˆ†æ Paper Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬è¦å‚è€ƒçš„å°±æ˜¯AlphaGo Zeroçš„è®ºæ–‡ã€Š[`Mastering the Game of Go without Human Knowledge`](https://web.archive.org/web/20171025100035/https://deepmind.com/documents/119/agz_unformatted_nature.pdf)ã€‹å’ŒAlphaZeroçš„è®ºæ–‡ã€Š[`Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm`](https://arxiv.org/pdf/1712.01815.pdf\n",
    ")ã€‹ã€‚\n",
    "\n",
    "What we want to refer to is AlphaGo Zero's paper \"[`Mastering the Game of Go without Human Knowledge`](https://web.archive.org/web/20171025100035/https://deepmind.com/documents/119/agz_unformatted_nature.pdf)\" and AlphaZero's paper \"[`Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm`](https://arxiv.org/pdf/1712.01815.pdf)\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å°å¼Ÿä¸æ‰ï¼ŒçŒ®ä¸‘ç¿»è¯‘äº†è¿™ä¸¤ç¯‡è®ºæ–‡ï¼Œæ—¶é—´ä»“ä¿ƒï¼Œæ°´å¹³æœ‰é™âœ§(â‰– â—¡ â‰–âœ¿)ï¼Œæ‚¨è¦æ˜¯çœ‹ä¸æƒ¯è‹±æ–‡ï¼Œå¸Œæœ›è¿™ä¸¤ç¯‡ç¿»è¯‘èƒ½æä¾›äº›è®¸å¸®åŠ©ã€‚\n",
    "\n",
    "Iâ€™m not talented, Xian Chou translated these two papers. The time is short and the level is limited âœ§(â‰– â—¡ â‰–âœ¿), if you are not familiar with English, I hope these two translations can provide some help.\n",
    "\n",
    "[`ã€ŠMastering the Game of Go without Human Knowledgeã€‹`](https://github.com/chengstone/cchess-zero/blob/master/Mastering_the_Game_of_Go_without_Human_Knowledge.ipynb)\n",
    "\n",
    "[`ã€ŠMastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithmã€‹`](https://github.com/chengstone/cchess-zero/blob/master/Mastering_Chess_and_Shogi_by_Self-Play_with_a_General_Reinforcement_Learning_Algorithm.ipynb)\n",
    "\n",
    "\n",
    "å»ºè®®åœ¨æœ¬åœ°ç”¨jupyter notebookæ‰“å¼€çœ‹ï¼Œæˆ‘å‘ç°ä»GitHubä¸Šçœ‹çš„è¯ï¼Œæœ‰äº›å…¬å¼æ²¡æœ‰æ˜¾ç¤ºå‡ºæ¥ï¼Œå¦å¤–å›¾ç‰‡ä¹Ÿæ²¡æœ‰æ˜¾ç¤ºå‡ºæ¥ã€‚\n",
    "\n",
    "It is recommended to use jupyter notebook to open it locally. I found that some formulas are not displayed when I look at it on GitHub, and the pictures are not displayed.\n",
    "\n",
    "(ycechungAI: thats why I am here to help =) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mastering the Game of Go without Human Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å…ˆä»ã€ŠMastering the Game of Go without Human Knowledgeã€‹è¯´èµ·ï¼Œç®—æ³•æ ¹æ®è¿™ç¯‡è®ºæ–‡æ¥å®ç°ï¼ŒAlphaZeroåªæœ‰å‡ ç‚¹ä¸åŒè€Œå·²ã€‚\n",
    "\n",
    "æ€»çš„æ¥è¯´ï¼ŒAlphaGo Zeroåˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼Œä¸€éƒ¨åˆ†æ˜¯MCTSï¼ˆè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼‰ï¼Œä¸€éƒ¨åˆ†æ˜¯ç¥ç»ç½‘ç»œã€‚\n",
    "\n",
    "æˆ‘ä»¬æ˜¯è¦æŠ›å¼ƒäººç±»æ£‹è°±çš„ï¼Œå­¦ä¼šå¦‚ä½•ä¸‹æ£‹å®Œå…¨æ˜¯é€šè¿‡è‡ªå¯¹å¼ˆæ¥å®Œæˆã€‚\n",
    "\n",
    "è¿‡ç¨‹æ˜¯è¿™æ ·ï¼Œé¦–å…ˆç”Ÿæˆæ£‹è°±ï¼Œç„¶åå°†æ£‹è°±ä½œä¸ºè¾“å…¥è®­ç»ƒç¥ç»ç½‘ç»œï¼Œè®­ç»ƒå¥½çš„ç¥ç»ç½‘ç»œç”¨æ¥é¢„æµ‹è½å­å’Œèƒœç‡ã€‚å¦‚ä¸‹å›¾ï¼š\n",
    "\n",
    "Let's start with \"Mastering the Game of Go without Human Knowledge\", the algorithm is implemented according to this paper, and AlphaZero has only a few differences.\n",
    "\n",
    "In general, AlphaGo Zero is divided into two parts, one is MCTS (Monte Carlo Tree Search), and the other is neural network.\n",
    "\n",
    "We are going to abandon human chess records, and learning how to play chess is done entirely through self-play.\n",
    "\n",
    "The process is like this, first generate a chess record, and then use the chess record as an input to train the neural network, and the trained neural network is used to predict the move and the winning rate. As shown below:\n",
    "\n",
    "![a1](./assets/a1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è’™ç‰¹å¡æ´›æ ‘æœç´¢ç®—æ³•  Monte Carlo Tree Search Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MCTSå°±æ˜¯ç”¨æ¥è‡ªå¯¹å¼ˆç”Ÿæˆæ£‹è°±çš„ï¼Œç»“åˆè®ºæ–‡ä¸­çš„å›¾ç¤ºè¿›è¡Œè¯´æ˜ï¼š\n",
    "MCTS is used to generate chess records from the game, combined with the illustrations in the paper to illustrate:\n",
    "![a2](./assets/a2.png)\n",
    "è®ºæ–‡ä¸­çš„æè¿°ï¼š\n",
    "\n",
    "AlphaGo Zeroä¸­çš„è’™ç‰¹å¡æ´›æ ‘æœç´¢ã€‚   Monte Carlo tree search in AlphaGo Zero.\n",
    "\n",
    "- a.æ¯æ¬¡æ¨¡æ‹Ÿé€šè¿‡é€‰æ‹©å…·æœ‰æœ€å¤§è¡ŒåŠ¨ä»·å€¼Qçš„è¾¹åŠ ä¸Šå–å†³äºæ‰€å­˜å‚¨çš„å…ˆéªŒæ¦‚ç‡På’Œè¯¥è¾¹çš„è®¿é—®è®¡æ•°Nï¼ˆæ¯æ¬¡è®¿é—®éƒ½è¢«å¢åŠ ä¸€æ¬¡ï¼‰çš„ä¸Šé™ç½®ä¿¡åŒºé—´Uæ¥éå†æ ‘ã€‚\n",
    "- b.å±•å¼€å¶å­èŠ‚ç‚¹ï¼Œé€šè¿‡ç¥ç»ç½‘ç»œ(P(s, Â·), V (s)) = $f_Î¸(s)$æ¥è¯„ä¼°å±€é¢sï¼›å‘é‡Pçš„å€¼å­˜å‚¨åœ¨å¶å­ç»“ç‚¹æ‰©å±•çš„è¾¹ä¸Šã€‚\n",
    "- c.æ›´æ–°è¡ŒåŠ¨ä»·å€¼Qç­‰äºåœ¨è¯¥è¡ŒåŠ¨ä¸‹çš„å­æ ‘ä¸­çš„æ‰€æœ‰è¯„ä¼°å€¼Vçš„å‡å€¼ã€‚\n",
    "- d.ä¸€æ—¦MCTSæœç´¢å®Œæˆï¼Œè¿”å›å±€é¢sä¸‹çš„è½å­æ¦‚ç‡Ï€ï¼Œä¸$N^{1 /Ï„}$æˆæ­£æ¯”ï¼Œå…¶ä¸­Næ˜¯ä»æ ¹çŠ¶æ€æ¯æ¬¡ç§»åŠ¨çš„è®¿é—®è®¡æ•°ï¼Œ Ï„æ˜¯æ§åˆ¶æ¸©åº¦çš„å‚æ•°ã€‚\n",
    "\n",
    "\n",
    "- a. Each simulation traverses the tree by selecting the edge with the largest action value Q plus the upper confidence interval U that depends on the stored prior probability P and the access count N of the edge (each access is increased once).\n",
    "- b. Expand the leaf nodes and evaluate the position s through the neural network (P(s, Â·), V (s)) = $f_Î¸(s)$; the value of the vector P is stored on the expanded edge of the leaf node.\n",
    "- c. The update action value Q is equal to the mean value of all the evaluation values â€‹â€‹V in the subtree under the action.\n",
    "- d. Once the MCTS search is completed, the probability Ï€ of returning to position s is proportional to $N^{1 /Ï„}$, where N is the count of visits for each move from the root state, and Ï„ is the parameter that controls the temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æŒ‰ç…§è®ºæ–‡æ‰€è¿°ï¼Œæ¯æ¬¡MCTSä½¿ç”¨1600æ¬¡æ¨¡æ‹Ÿã€‚è¿‡ç¨‹æ˜¯è¿™æ ·çš„ï¼Œç°åœ¨AIä»ç™½æ¿ä¸€å—å¼€å§‹è‡ªå·±è·Ÿè‡ªå·±ä¸‹æ£‹ï¼ŒåªçŸ¥é“è§„åˆ™ï¼Œä¸çŸ¥é“å¥—è·¯ï¼Œé‚£åªå¥½ä¹±ä¸‹ã€‚æ¯ä¸‹ä¸€æ­¥æ£‹ï¼Œéƒ½è¦é€šè¿‡MCTSæ¨¡æ‹Ÿ1600æ¬¡ä¸Šå›¾ä¸­çš„a~cï¼Œä»è€Œå¾—å‡ºæˆ‘è¿™æ¬¡è¦æ€ä¹ˆèµ°å­ã€‚\n",
    "\n",
    "According to the paper, each MCTS uses 1600 simulations. The process is like this. Now the AI â€‹â€‹starts from the whiteboard and plays chess with itself. It only knows the rules and doesn't know the routines, so it has no choice but to mess around. For each next move, I have to simulate 1600 times a~c in the above picture through MCTS, so as to figure out how I am going to move this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¥è¯´è¯´a~cï¼ŒMCTSæœ¬è´¨ä¸Šæ˜¯æˆ‘ä»¬æ¥ç»´æŠ¤ä¸€æ£µæ ‘ï¼Œè¿™æ£µæ ‘çš„æ¯ä¸ªèŠ‚ç‚¹ä¿å­˜äº†æ¯ä¸€ä¸ªå±€é¢ï¼ˆsituationï¼‰è¯¥å¦‚ä½•èµ°å­ï¼ˆactionï¼‰çš„ä¿¡æ¯ã€‚è¿™äº›ä¿¡æ¯æ˜¯ï¼ŒN(s, a)æ˜¯è®¿é—®æ¬¡æ•°ï¼ŒW(s, a)æ˜¯æ€»è¡ŒåŠ¨ä»·å€¼ï¼ŒQ(s, a)æ˜¯å¹³å‡è¡ŒåŠ¨ä»·å€¼ï¼ŒP(s, a)æ˜¯è¢«é€‰æ‹©çš„æ¦‚ç‡ã€‚\n",
    "\n",
    "Letâ€™s talk about a~c, MCTS essentially means that we maintain a tree. Each node of this tree saves information on how to move each situation. The information is that N(s, a) is the number of visits, W(s, a) is the total action value, Q(s, a) is the average action value, and P(s, a) is the probability of being selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¯æ¬¡æ¨¡æ‹Ÿçš„è¿‡ç¨‹éƒ½ä¸€æ ·ï¼Œä»çˆ¶èŠ‚ç‚¹çš„å±€é¢å¼€å§‹ï¼Œé€‰æ‹©ä¸€ä¸ªèµ°å­ã€‚æ¯”å¦‚å¼€å±€çš„æ—¶å€™ï¼Œæ‰€æœ‰åˆæ³•çš„èµ°å­éƒ½æ˜¯å¯èƒ½çš„é€‰æ‹©ï¼Œé‚£ä¹ˆæˆ‘è¯¥é€‰å“ªä¸ªèµ°å­å‘¢ï¼Ÿè¿™å°±æ˜¯selectè¦åšçš„äº‹æƒ…ã€‚MCTSé€‰æ‹©Q(s, a) + U(s, a)æœ€å¤§çš„é‚£ä¸ªactionã€‚Qçš„å…¬å¼ä¸€ä¼šåœ¨Backupä¸­æè¿°ã€‚Uçš„å…¬å¼å¦‚ä¸‹ï¼š\n",
    "\n",
    "The process of each simulation is the same, starting from the position of the parent node, choose a move. For example, at the beginning of the game, all legal moves are possible choices, so which move should I choose? This is what select does. MCTS selects the action with the largest Q(s, a) + U(s, a). Q's formula will be described in Backup. The formula of U is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$U(s,a) = c_{puct} P(s,a)  \\frac{\\sqrt {\\sum_b N(s,b)}}{1 + N(s,a)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™ä¸ªå¯ä»¥ç†è§£æˆï¼šU(s, a)  = $c_{puct}$ Ã— æ¦‚ç‡P(s, a)  Ã— np.sqrt(çˆ¶èŠ‚ç‚¹è®¿é—®æ¬¡æ•°N) / ( 1 + æŸå­èŠ‚ç‚¹actionçš„è®¿é—®æ¬¡æ•°N(s, a) )\n",
    "\n",
    "ç”¨è®ºæ–‡ä¸­çš„è¯è¯´ï¼Œc_puctæ˜¯ä¸€ä¸ªå†³å®šæ¢ç´¢æ°´å¹³çš„å¸¸æ•°ï¼›è¿™ç§æœç´¢æ§åˆ¶ç­–ç•¥æœ€åˆå€¾å‘äºå…·æœ‰é«˜å…ˆéªŒæ¦‚ç‡å’Œä½è®¿é—®æ¬¡æ•°çš„è¡Œä¸ºï¼Œä½†æ˜¯æ¸è¿‘åœ°å€¾å‘äºå…·æœ‰é«˜è¡ŒåŠ¨ä»·å€¼çš„è¡Œä¸ºã€‚\n",
    "\n",
    "This can be understood as: U(s, a) = $c_{puct}$ Ã— probability P(s, a) Ã— np.sqrt (the number of parent node visits N) / (1 + the number of child node action visits N( s, a))\n",
    "\n",
    "In the words of the paper, c_puct is a constant that determines the level of exploration; this search control strategy initially tends to have high prior probability and low number of visits, but asymptotically tends to have high action value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è®¡ç®—è¿‡åï¼Œæˆ‘å°±çŸ¥é“å½“å‰å±€é¢ä¸‹ï¼Œå“ªä¸ªactionçš„Q+Uå€¼æœ€å¤§ï¼Œé‚£è¿™ä¸ªactionèµ°å­ä¹‹åçš„å±€é¢å°±æ˜¯ç¬¬äºŒæ¬¡æ¨¡æ‹Ÿçš„å½“å‰å±€é¢ã€‚æ¯”å¦‚å¼€å±€ï¼ŒQ+Uæœ€å¤§çš„æ˜¯å½“å¤´ç‚®ï¼Œç„¶åæˆ‘å°±Selectå½“å¤´ç‚®è¿™ä¸ªactionï¼Œå†ä¸‹ä¸€æ¬¡Selectå°±ä»å½“å¤´ç‚®çš„è¿™ä¸ªæ£‹å±€é€‰æ‹©ä¸‹ä¸€ä¸ªèµ°å­ã€‚\n",
    "\n",
    "After the calculation, I know which action has the largest Q+U value in the current situation, and the situation after this action moves is the current situation of the second simulation. For example, in the opening game, Q+U is the biggest shot, and then I select the action of the top shot, and the next time Select will choose the next move from the top shot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Expand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨å¼€å§‹ç¬¬äºŒæ¬¡æ¨¡æ‹Ÿäº†ï¼Œå‡å¦‚ä¹‹å‰çš„actionæ˜¯å½“å¤´ç‚®ï¼Œæˆ‘ä»¬è¦æ¥ç€è¿™ä¸ªå±€é¢é€‰æ‹©actionï¼Œä½†æ˜¯è¿™ä¸ªå±€é¢æ˜¯ä¸ªå¶å­èŠ‚ç‚¹ã€‚å°±æ˜¯è¯´å½“å¤´ç‚®ä¹‹åå¯ä»¥é€‰æ‹©å“ªäº›actionä¸çŸ¥é“ï¼Œè¿™æ ·å°±éœ€è¦expandäº†ï¼Œé€šè¿‡expandå¾—åˆ°ä¸€ç³»åˆ—å¯èƒ½çš„actionèŠ‚ç‚¹ã€‚è¿™æ ·å®é™…ä¸Šå°±æ˜¯åœ¨æ‰©å±•è¿™æ£µæ ‘ï¼Œä»åªæœ‰æ ¹èŠ‚ç‚¹å¼€å§‹ï¼Œä¸€ç‚¹ä¸€ç‚¹çš„æ‰©å±•ã€‚\n",
    "\n",
    "Now itâ€™s the second simulation. If the previous action was the headshot, we have to select the action following this situation, but this situation is a leaf node. That is to say, you can choose which actions you can choose after you donâ€™t know, so you need to expand, and you can get a series of possible action nodes through expand. This is actually expanding the tree, starting with only the root node and expanding little by little."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expand and evaluateè¿™ä¸ªéƒ¨åˆ†æœ‰ä¸ªéœ€è¦å…³æ³¨çš„åœ°æ–¹ã€‚è®ºæ–‡ä¸­è¯´ï¼šåœ¨é˜Ÿåˆ—ä¸­çš„å±€é¢ç”±ç¥ç»ç½‘ç»œä½¿ç”¨æœ€å°æ‰¹é‡mini-batch å¤§å°ä¸º8è¿›è¡Œè¯„ä¼°ï¼›æœç´¢çº¿ç¨‹è¢«é”å®šï¼Œç›´åˆ°è¯„ä¼°å®Œæˆã€‚å¶å­èŠ‚ç‚¹è¢«å±•å¼€ï¼Œæ¯ä¸ªè¾¹($s_L$, a)è¢«åˆå§‹åŒ–ä¸º{N($s_L$, a) = 0ï¼ŒW($s_L$, a) = 0ï¼ŒQ($s_L$, a) = 0ï¼ŒP($s_L$, a) = $p_a$} ç„¶å**å€¼vè¢«å›ä¼ ï¼ˆbacked upï¼‰**ã€‚\n",
    "\n",
    "There is a place to pay attention to in the Expand and evaluate section. The paper said: The situation in the queue is evaluated by the neural network using a minimum batch size of 8; the search thread is locked until the evaluation is completed. The leaf node is expanded, and each edge (ğ‘ ğ¿\n",
    ", a) is initialized to {N(ğ‘ ğ¿, a) = 0, W(ğ‘ ğ¿, a) = 0, Q(ğ‘ ğ¿, a) = 0, P(ğ‘ ğ¿, a) = ğ‘ğ‘} Then the value v is returned ( backed up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœæˆ‘å½“å‰çš„å±€é¢æ²¡æœ‰è¢«expandè¿‡ï¼Œä¸çŸ¥é“ä¸‹ä¸€æ­¥è¯¥æ€ä¹ˆä¸‹ï¼Œæ‰€ä»¥è¦expandï¼Œè¿™ä¸ªæ—¶å€™è¦ç”¨æˆ‘ä»¬çš„ç¥ç»ç½‘ç»œå‡ºé©¬ã€‚æŠŠå½“å‰çš„å±€é¢ä½œä¸ºè¾“å…¥ä¼ ç»™ç¥ç»ç½‘ç»œï¼Œç¥ç»ç½‘ç»œä¼šè¿”å›ç»™æˆ‘ä»¬ä¸€ä¸ªactionå‘é‡på’Œå½“å‰èƒœç‡vã€‚å…¶ä¸­actionå‘é‡æ˜¯å½“å‰å±€é¢æ¯ä¸ªåˆæ³•actionçš„èµ°å­æ¦‚ç‡ã€‚å½“ç„¶ï¼Œå› ä¸ºç¥ç»ç½‘ç»œè¿˜æ²¡æœ‰è®­ç»ƒå¥½ï¼Œè¾“å‡ºä½œä¸ºå‚è€ƒæ·»åŠ åˆ°æˆ‘ä»¬çš„è’™ç‰¹å¡æ´›æ ‘ä¸Šã€‚è¿™æ ·åœ¨å½“å‰å±€é¢ä¸‹ï¼Œæ‰€æœ‰å¯èµ°çš„actionä»¥åŠå¯¹åº”çš„æ¦‚ç‡på°±éƒ½æœ‰äº†ï¼Œæ¯ä¸ªæ–°å¢çš„actionèŠ‚ç‚¹éƒ½æŒ‰ç…§è®ºæ–‡ä¸­è¯´çš„å¯¹è‹¥å¹²ä¿¡æ¯èµ‹å€¼ï¼Œ{N($s_L$, a) = 0ï¼ŒW($s_L$, a) = 0ï¼ŒQ($s_L$, a) = 0ï¼ŒP($s_L$, a) = $p_a$} ã€‚è¿™äº›æ–°å¢çš„èŠ‚ç‚¹ä½œä¸ºå½“å‰å±€é¢èŠ‚ç‚¹çš„å­èŠ‚ç‚¹ã€‚\n",
    "\n",
    "If my current situation has not been expanded, I donâ€™t know what to do next, so I need to expand. At this time, we need to use our neural network. Pass the current situation as input to the neural network, and the neural network will return us an action vector p and the current winning percentage v. The action vector is the move probability of each legal action in the current situation. Of course, because the neural network has not been trained yet, the output is added to our Monte Carlo tree as a reference. In this way, in the current situation, all the actions that can be taken and the corresponding probability p are there, and each new action node is assigned a number of information according to the paper, {N($s_L$, a) = 0 , W($s_L$, a) = 0, Q($s_L$, a) = 0, P($s_L$, a) = $p_a$}. These newly added nodes serve as child nodes of the current situation node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Backup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¥ä¸‹æ¥å°±æ˜¯é‡ç‚¹ï¼Œevaluateå’ŒBackupä¸€èµ·è¯´ï¼Œå…ˆçœ‹çœ‹Backupåšä»€ä¹ˆäº‹å§ï¼šè¾¹çš„ç»Ÿè®¡æ•°æ®åœ¨æ¯ä¸€æ­¥tâ‰¤Lä¸­åå‘æ›´æ–°ã€‚è®¿é—®è®¡æ•°é€’å¢ï¼Œ$N(s_t , a_t) = N(s_t , a_t) +1$ï¼Œå¹¶ä¸”åŠ¨ä½œä»·å€¼æ›´æ–°ä¸ºå¹³å‡å€¼ï¼Œ $W(s_t , a_t) = W(s_t , a_t) + vï¼ŒQ(s_t , a_t) = \\frac{W(s_t  ,a_t)}{N(s_t  ,a_t)}$ã€‚æˆ‘ä»¬ä½¿ç”¨**è™šæ‹ŸæŸå¤±**æ¥ç¡®ä¿æ¯ä¸ªçº¿ç¨‹è¯„ä¼°ä¸åŒçš„èŠ‚ç‚¹ã€‚\n",
    "\n",
    "The next step is the key point. Evaluate and Backup said together, letâ€™s take a look at what Backup does: the statistical data of the edge is updated in reverse at each step tâ‰¤L. The visit count is incremented, $N(s_t, a_t) = N(s_t, a_t) +1$, and the action value is updated to the average value, $W(s_t, a_t) = W(s_t, a_t) + v, Q(s_t , a_t) = \\frac{W(s_t ,a_t)}{N(s_t ,a_t)}$. We use **virtual loss** to ensure that each thread evaluates different nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬æ¥æ•´ç†ä¸€ä¸‹æ€è·¯ï¼Œä»»æ„ä¸€ä¸ªå±€é¢ï¼ˆå°±æ˜¯èŠ‚ç‚¹ï¼‰ï¼Œè¦ä¹ˆè¢«å±•å¼€è¿‡ï¼ˆexpandï¼‰ï¼Œè¦ä¹ˆæ²¡æœ‰å±•å¼€è¿‡ï¼ˆå°±æ˜¯å¶å­èŠ‚ç‚¹ï¼‰ã€‚å±•å¼€è¿‡çš„èŠ‚ç‚¹å¯ä»¥ä½¿ç”¨Selecté€‰æ‹©åŠ¨ä½œè¿›å…¥ä¸‹ä¸€ä¸ªå±€é¢ï¼Œä¸‹ä¸€ä¸ªå±€é¢ä»ç„¶æ˜¯è¿™ä¸ªè¿‡ç¨‹ï¼Œå¦‚æœå±•å¼€è¿‡è¿˜æ˜¯å¯ä»¥é€šè¿‡Selectè¿›å…¥ä¸‹ä¸‹ä¸ªå±€é¢ï¼Œè¿™ä¸ªè¿‡ç¨‹ä¸€ç›´æŒç»­ä¸‹å»ç›´åˆ°è¿™ç›˜æ£‹åˆ†å‡ºèƒœå¹³è´Ÿäº†ï¼Œæˆ–è€…é‡åˆ°æŸä¸ªå±€é¢æ²¡æœ‰è¢«å±•å¼€è¿‡ä¸ºæ­¢ã€‚\n",
    "\n",
    "Let's sort out our thoughts, any situation (that is, a node), either has been expanded (expand) or has not been expanded (that is, a leaf node). The node that has been expanded can use the Select action to enter the next position. The next position is still this process. If it has been expanded, you can still enter the next position through Select. This process will continue until the game is won. , Or encounter a situation that has not been unfolded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœæ²¡æœ‰å±•å¼€è¿‡ï¼Œé‚£ä¹ˆæ‰§è¡Œexpandæ“ä½œï¼Œé€šè¿‡ç¥ç»ç½‘ç»œå¾—åˆ°æ¯ä¸ªåŠ¨ä½œçš„æ¦‚ç‡å’Œèƒœç‡vï¼ŒæŠŠè¿™äº›åŠ¨ä½œæ·»åŠ åˆ°æ ‘ä¸Šï¼Œæœ€åæŠŠèƒœç‡**vå›ä¼ ï¼ˆbacked upï¼‰**ï¼Œbacked upç»™è°ï¼Ÿ\n",
    "\n",
    "If it has not been expanded, then perform the expand operation, get the probability and winning rate v of each action through the neural network, add these actions to the tree, and finally return the winning rate **v backed up**, who is backed up to? ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬çŸ¥é“è¿™å…¶å®æ˜¯ä¸€è·¯é€’å½’ä¸‹å»çš„è¿‡ç¨‹ï¼Œä¸€ç›´åœ¨Selectï¼Œé€’å½’å¿…é¡»è¦æœ‰ç»“æŸæ¡ä»¶ï¼Œä¸ç„¶å°±æ˜¯æ­»å¾ªç¯äº†ã€‚æ‰€ä»¥åˆ†å‡ºèƒœè´Ÿå’Œé‡åˆ°å¶å­èŠ‚ç‚¹å°±æ˜¯é€’å½’ç»“æŸæ¡ä»¶ï¼ŒæŠŠèƒœç‡væˆ–è€…åˆ†å‡ºçš„èƒœå¹³è´Ÿvalueä½œä¸ºè¿”å›å€¼ï¼Œå›ä¼ ç»™ä¸Šä¸€å±‚ã€‚\n",
    "\n",
    "We know that this is actually a process of recursion all the way, always in Select, recursion must have an end condition, otherwise it will be an endless loop. Therefore, dividing the victory and encountering the leaf node is the recursive end condition, and the victory rate v or the divided victory and loss value is used as the return value and passed back to the upper layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™ä¸ªè¿‡ç¨‹å°±æ˜¯evaluateï¼Œæ˜¯ä¸ºäº†Backupæ­¥éª¤åšå‡†å¤‡ã€‚å› ä¸ºåœ¨Backupæ­¥éª¤ï¼Œæˆ‘ä»¬è¦ç”¨væ¥æ›´æ–°Wå’ŒQçš„ï¼Œä½†æ˜¯å¦‚æœåªåšäº†ä¸€æ¬¡Selectï¼Œæ£‹å±€è¿˜æ²¡æœ‰ç»“æŸï¼Œæ­¤æ—¶çš„væ˜¯ä¸æ˜ç¡®çš„ï¼Œå¿…é¡»è¦ç­‰åˆ°ä¸€ç›˜æ£‹å®Œæ•´çš„ä¸‹å®Œæ‰èƒ½çŸ¥é“våˆ°åº•æ˜¯å¤šå°‘ã€‚å°±æ˜¯è¯´æˆ‘ç°åœ¨ä¸‹äº†ä¸€æ­¥æ£‹ï¼Œä¸ç®¡è¿™æ­¥æ£‹æ˜¯å¥½æ£‹è¿˜æ˜¯è‡­æ£‹ï¼Œåªæœ‰ä¸‹å®Œæ•´ç›˜æœŸåˆ†å‡ºèƒœè´Ÿï¼Œæ‰èƒ½ç»™æˆ‘ä¸‹çš„è¿™æ­¥æ£‹è¯„åˆ†ã€‚ä¸ç®¡è¿™æ­¥æ£‹çš„å¾—å¤±ï¼Œå³ä½¿æˆ‘è¿™æ­¥æ£‹ä¸¢äº†ä¸ªè½¦ï¼Œä½†æœ€åæˆ‘èµ¢äº†ï¼Œé‚£è¿™ä¸ªvå°±æ˜¯ç§¯æçš„ã€‚åŒæ ·å³ä½¿æˆ‘è¿™æ­¥æ£‹åƒäº†å¯¹æ–¹ä¸€ä¸ªå­ï¼Œä½†æœ€åè¾“æ£‹äº†ï¼Œä¹Ÿä¸èƒ½è®¤ä¸ºæˆ‘è¿™æ­¥æ£‹å°±æ˜¯å¥½æ£‹ã€‚\n",
    "\n",
    "This process is evaluate, which is to prepare for the Backup step. Because in the Backup step, we need to use v to update W and Q, but if we only do Select once, the game is not over yet. At this time, v is not clear, and we must wait until a game of chess is completed to know the end of v. how many. That is to say, I have made a move now, no matter whether it is a good move or a bad move, only by playing a complete game period can I score the move. Regardless of the pros and cons of this move, even if I lose a rook in this move, but in the end I win, then this v is positive. Similarly, even if I ate the opponent's piece in this move, but lost in the end, I can't think that my move is a good move."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç”¨ä¸€å¹…å›¾æ¦‚æ‹¬ä¸€ä¸‹è¿™ä¸ªè¿‡ç¨‹ï¼š   Use a picture to summarize this process:\n",
    "![c1](./assets/c1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å½“å€¼è¢«å›ä¼ ï¼Œå°±è¦åšBackupäº†ï¼Œè¿™é‡Œå¾ˆå…³é”®ã€‚å› ä¸ºæˆ‘ä»¬æ˜¯å¤šçº¿ç¨‹åŒæ—¶åœ¨åšMCTSï¼Œç”±äºSelectç®—æ³•éƒ½ä¸€æ ·ï¼Œéƒ½æ˜¯é€‰æ‹©Q+Uæœ€å¤§èŠ‚ç‚¹ï¼Œæ‰€ä»¥å¾ˆæœ‰å¯èƒ½æ‰€æœ‰çš„çº¿ç¨‹æœ€ç»ˆé€‰æ‹©çš„æ˜¯åŒä¸€ä¸ªèŠ‚ç‚¹ï¼Œè¿™å°±å°´å°¬äº†ã€‚æˆ‘ä»¬çš„ç›®çš„æ˜¯å°½å¯èƒ½åœ¨æ ‘ä¸Šæœç´¢å‡ºå„ç§ä¸åŒçš„ç€æ³•ï¼Œæœ€ç»ˆé€‰æ‹©ä¸€æ­¥å¥½æ£‹ï¼Œæ€ä¹ˆåŠå‘¢ï¼Ÿè®ºæ–‡ä¸­å·²ç»ç»™å‡ºäº†åŠæ³•ï¼Œâ€œæˆ‘ä»¬ä½¿ç”¨**è™šæ‹ŸæŸå¤±**æ¥ç¡®ä¿æ¯ä¸ªçº¿ç¨‹è¯„ä¼°ä¸åŒçš„èŠ‚ç‚¹ã€‚â€\n",
    "\n",
    "When the value is returned, backup is required, which is very important here. Because we are doing MCTS with multiple threads at the same time, and because the Select algorithm is the same, we choose the largest node of Q+U, so it is very likely that all threads will eventually choose the same node, which is embarrassing. Our goal is to search for various moves in the tree as much as possible, and finally choose a good move. What should we do? The method has been given in the paper, \"We use **virtual loss** to ensure that each thread evaluates different nodes.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å°±æ˜¯è¯´ï¼Œé€šè¿‡Selecté€‰å‡ºæŸèŠ‚ç‚¹åï¼Œäººä¸ºå¢å¤§è¿™ä¸ªèŠ‚ç‚¹çš„è®¿é—®æ¬¡æ•°Nï¼Œå¹¶å‡å°‘èŠ‚ç‚¹çš„æ€»è¡ŒåŠ¨ä»·å€¼Wï¼Œå› ä¸ºå¹³å‡è¡ŒåŠ¨ä»·å€¼Q = W / Nï¼Œè¿™æ ·åˆ†å­å‡å°‘ï¼Œåˆ†æ¯å¢åŠ ï¼Œå°±å‡å°‘äº†Qå€¼ï¼Œè¿™æ ·é€’å½’è¿›è¡Œçš„æ—¶å€™ï¼Œæ­¤èŠ‚ç‚¹çš„Q+Uä¸æ˜¯æœ€å¤§ï¼Œé¿å…è¢«é€‰ä¸­ï¼Œè®©å…¶ä»–çš„çº¿ç¨‹å°è¯•é€‰æ‹©åˆ«çš„èŠ‚ç‚¹è¿›è¡Œæ ‘æœç´¢ã€‚è¿™ä¸ªäººä¸ºå¢åŠ å’Œå‡å°‘çš„é‡å°±æ˜¯è™šæ‹ŸæŸå¤±virtual lossã€‚\n",
    "\n",
    "That is to say, after selecting a node through Select, the number of visits of this node N is artificially increased, and the total action value W of the node is reduced, because the average action value Q = W / N, so that the numerator decreases and the denominator increases, so it decreases Q value, so when recursively, the Q+U of this node is not the largest, avoid being selected, and let other threads try to select other nodes for tree search. This artificial increase and decrease is the virtual loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨MCTSçš„è¿‡ç¨‹è¶Šæ¥è¶Šæ¸…æ™°äº†ï¼ŒSelecté€‰æ‹©èŠ‚ç‚¹ï¼Œé€‰æ‹©åï¼Œå¯¹å½“å‰èŠ‚ç‚¹ä½¿ç”¨è™šæ‹ŸæŸå¤±ï¼Œé€šè¿‡é€’å½’ç»§ç»­Selectï¼Œç›´åˆ°åˆ†å‡ºèƒœè´Ÿæˆ–ExpandèŠ‚ç‚¹ï¼Œå¾—åˆ°è¿”å›å€¼valueã€‚ç°åœ¨å°±å¯ä»¥ä½¿ç”¨valueè¿›è¡ŒBackupäº†ï¼Œä½†é¦–å…ˆè¦è¿˜åŸWå’ŒNï¼Œä¹‹å‰Nå¢åŠ äº†è™šæ‹ŸæŸå¤±ï¼Œè¿™æ¬¡è¦å‡å›å»ï¼Œä¹‹å‰å‡å°‘äº†è™šæ‹ŸæŸå¤±çš„Wä¹Ÿè¦åŠ å›æ¥ã€‚\n",
    "\n",
    "Now the process of MCTS is getting clearer and clearer. Select selects a node. After selection, virtual loss is applied to the current node. Select is continued through recursion until the winner or Expand node is determined, and the return value value is obtained. Now you can use value to perform backup, but first you need to restore W and N. Before N increased the virtual loss, this time you need to reduce it, and the W that previously reduced the virtual loss should also be added back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç„¶åå¼€å§‹åšBackupï¼Œâ€œè¾¹çš„ç»Ÿè®¡æ•°æ®åœ¨æ¯ä¸€æ­¥tâ‰¤Lä¸­åå‘æ›´æ–°ã€‚è®¿é—®è®¡æ•°é€’å¢ï¼Œ$N(s_t , a_t) = N(s_t , a_t) +1$ï¼Œå¹¶ä¸”åŠ¨ä½œä»·å€¼æ›´æ–°ä¸ºå¹³å‡å€¼ï¼Œ $W(s_t , a_t) = W(s_t , a_t) + vï¼ŒQ(s_t , a_t) = \\frac{W(s_t  ,a_t)}{N(s_t  ,a_t)}$ã€‚â€ï¼Œè¿™äº›ä¸ç”¨æˆ‘å†è§£é‡Šäº†å§ï¼ŸåŒæ—¶æˆ‘ä»¬è¿˜è¦æ›´æ–°Uï¼ŒUçš„å…¬å¼ä¸Šé¢ç»™å‡ºè¿‡ã€‚è¿™ä¸ªåå‘æ›´æ–°ï¼Œå…¶å®å°±æ˜¯é€’å½’çš„æŠŠå€¼è¿”å›å›å»ã€‚æœ‰ä¸€ç‚¹ä¸€å®šè¦**æ³¨æ„ï¼Œå°±æ˜¯æˆ‘ä»¬çš„è¿”å›å€¼ä¸€å®šè¦ç¬¦å·åè½¬**ï¼Œæ€ä¹ˆç†è§£ï¼Ÿå°±æ˜¯è¯´å¯¹äºå½“å‰èŠ‚ç‚¹æ˜¯èƒœï¼Œé‚£ä¹ˆå¯¹äºä¸Šä¸€ä¸ªèŠ‚ç‚¹ä¸€å®šæ˜¯è´Ÿï¼Œæ˜ç™½è¿™ä¸ªæ„æ€äº†å§ï¼Ÿæ‰€ä»¥è¿”å›çš„æ˜¯-valueã€‚\n",
    "\n",
    "Then start to do Backup, \"The statistical data of the edge is updated in the reverse direction at each step tâ‰¤L. The access count increases, $N(s_t, a_t) = N(s_t, a_t) +1$, and the action value is updated to the average value , $W(s_t, a_t) = W(s_t, a_t) + v, Q(s_t, a_t) = \\frac{W(s_t ,a_t)}{N(s_t ,a_t)}$.\", these donâ€™t need me Explain it again? At the same time, we have to update U, the formula of U is given above. This reverse update is actually returning the value back recursively. One thing must be **attention, that is, our return value must be sign inverted**, how to understand? In other words, it is a win for the current node, so it must be a loss for the previous node. Do you understand this? So what is returned is -value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æŒ‰ç…§ä¸Šè¿°è¿‡ç¨‹æ‰§è¡Œa~cï¼Œè®ºæ–‡ä¸­æ˜¯æ¯æ­¥æ£‹æ‰§è¡Œ1600æ¬¡æ¨¡æ‹Ÿï¼Œé‚£å°±æ˜¯1600æ¬¡çš„a~cï¼Œè¿™ä¸ªMCTSçš„è¿‡ç¨‹å°±æ˜¯æ¨¡æ‹Ÿè‡ªæˆ‘å¯¹å¼ˆçš„è¿‡ç¨‹ã€‚æ¨¡æ‹Ÿç»“æŸåï¼ŒåŸºæœ¬ä¸Šèƒ½è¦†ç›–å¤§å¤šæ•°çš„æ£‹å±€å’Œç€æ³•ï¼Œæ¯æ­¥æ£‹è¯¥æ€ä¹ˆä¸‹ï¼Œä¸‹å®Œä»¥åèƒœç‡æ˜¯å¤šå°‘ï¼Œå¾—åˆ°ä»€ä¹ˆæ ·çš„å±€é¢éƒ½èƒ½åœ¨æ ‘ä¸Šæ‰¾åˆ°ã€‚ç„¶åä»æ ‘ä¸Šé€‰æ‹©å½“å‰å±€é¢åº”è¯¥ä¸‹å“ªä¸€æ­¥æ£‹ï¼Œè¿™å°±æ˜¯æ­¥éª¤d.play:\"åœ¨æœç´¢ç»“æŸæ—¶ï¼ŒAlphaGo Zeroåœ¨æ ¹èŠ‚ç‚¹s0é€‰æ‹©ä¸€ä¸ªèµ°å­aï¼Œä¸å…¶è®¿é—®è®¡æ•°å¹‚æŒ‡æ•°æˆæ­£æ¯”ï¼Œ$Ï€(a|s_0) = \\frac{N(s_0,a) ^{1/Ï„}}{\\sum_b N(s_0,b)^{1/Ï„}}$ ï¼Œå…¶ä¸­Ï„æ˜¯æ§åˆ¶æ¢ç´¢æ°´å¹³çš„æ¸©åº¦å‚æ•°ã€‚åœ¨éšåçš„æ—¶é—´æ­¥é‡æ–°ä½¿ç”¨æœç´¢æ ‘ï¼šä¸æ‰€èµ°å­çš„åŠ¨ä½œå¯¹åº”çš„å­èŠ‚ç‚¹æˆä¸ºæ–°çš„æ ¹èŠ‚ç‚¹ï¼›ä¿ç•™è¿™ä¸ªèŠ‚ç‚¹ä¸‹é¢çš„å­æ ‘æ‰€æœ‰çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œè€Œæ ‘çš„å…¶ä½™éƒ¨åˆ†è¢«ä¸¢å¼ƒã€‚å¦‚æœæ ¹èŠ‚ç‚¹çš„ä»·å€¼å’Œæœ€å¥½çš„å­èŠ‚ç‚¹ä»·å€¼ä½äºé˜ˆå€¼$v_{resign}$ï¼Œåˆ™AlphaGo Zeroä¼šè®¤è¾“ã€‚\"\n",
    "\n",
    "Follow the above process to execute a~c. In the paper, 1600 simulations are executed for each move, that is, 1600 a~c. This MCTS process is the process of simulating self-play. After the simulation is over, it can basically cover most of the chess games and moves, how to play each move, what is the winning rate after playing, and what kind of position you get can be found in the tree. Then select from the tree which move should be played in the current position. This is step d.play: \"At the end of the search, AlphaGo Zero selects a move a at the root node s0, which is proportional to its visit count power exponent, $Ï€(a |s_0) = \\frac{N(s_0,a) ^{1/Ï„}}{\\sum_b N(s_0,b)^{1/Ï„}}$, where Ï„ is the temperature parameter that controls the exploration level. In the following The search tree is reused at the time step: the child node corresponding to the action of the move becomes the new root node; all the statistical information of the subtree below this node is retained, and the rest of the tree is discarded. If the value of the root node and If the value of the best child node is lower than the threshold $v_{resign}$, AlphaGo Zero will admit defeat.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å½“æ¨¡æ‹Ÿç»“æŸåï¼Œå¯¹äºå½“å‰å±€é¢ï¼ˆå°±æ˜¯æ ‘çš„æ ¹èŠ‚ç‚¹ï¼‰çš„æ‰€æœ‰å­èŠ‚ç‚¹å°±æ˜¯æ¯ä¸€æ­¥å¯¹åº”çš„actionèŠ‚ç‚¹ï¼Œé€‰æ‹©å“ªä¸€ä¸ªactionå‘¢ï¼ŸæŒ‰ç…§è®ºæ–‡æ‰€è¯´æ˜¯é€šè¿‡è®¿é—®è®¡æ•°Næ¥ç¡®å®šçš„ã€‚è¿™ä¸ªå¥½ç†è§£å§ï¼Ÿå®ç°ä¸Šä¹Ÿå®¹æ˜“ï¼Œå½“å‰èŠ‚ç‚¹çš„æ‰€æœ‰èŠ‚ç‚¹æ˜¯å¯ä»¥è·å¾—çš„ï¼Œæ¯ä¸ªå­èŠ‚ç‚¹çš„ä¿¡æ¯Néƒ½å¯ä»¥è·å¾—ï¼Œç„¶åä»å¤šä¸ªactionä¸­é€‰ä¸€ä¸ªï¼Œè¿™å…¶å®æ˜¯å¤šåˆ†ç±»é—®é¢˜ã€‚æˆ‘ä»¬ä½¿ç”¨softmaxæ¥å¾—åˆ°é€‰æ‹©æŸä¸ªactionçš„æ¦‚ç‡ï¼Œä¼ ç»™softmaxçš„æ˜¯æ¯ä¸ªactionçš„logitsï¼ˆ$N(s_0,a) ^{1/Ï„}$ï¼‰,è¿™å…¶å®å¯ä»¥æ”¹æˆ$1/Ï„ * log(N(s_0,a))$ã€‚è¿™æ ·å°±å¾—åˆ°äº†å½“å‰å±€é¢æ‰€æœ‰å¯é€‰actionçš„æ¦‚ç‡å‘é‡ï¼Œæœ€ç»ˆé€‰æ‹©æ¦‚ç‡æœ€å¤§çš„é‚£ä¸ªactionä½œä¸ºè¦ä¸‹çš„ä¸€æ­¥æ£‹ï¼Œå¹¶ä¸”å°†è¿™ä¸ªé€‰æ‹©çš„èŠ‚ç‚¹ä½œä¸ºæ ‘çš„æ ¹èŠ‚ç‚¹ã€‚\n",
    "\n",
    "When the simulation is over, all the child nodes of the current situation (that is, the root node of the tree) are the action nodes corresponding to each step. Which action should be selected? According to the paper, it is determined by the access count N. Is this easy to understand? It is also easy to implement. All nodes of the current node can be obtained, and the information N of each child node can be obtained, and then select one from multiple actions. This is actually a multi-classification problem. We use softmax to get the probability of choosing an action. What is passed to softmax is the logits of each action ($N(s_0,a) ^{1/Ï„}$), which can actually be changed to $1/Ï„ * log( N(s_0,a))$. In this way, the probability vectors of all optional actions in the current situation are obtained, and finally the action with the highest probability is selected as the move to be played, and the selected node is regarded as the root node of the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æŒ‰ç…§å›¾1ä¸­a.Self-Playçš„è¯´æ³•å°±æ˜¯ï¼Œä»å±€é¢$s_t$è¿›è¡Œè‡ªæˆ‘å¯¹å¼ˆçš„æ ‘æœç´¢ï¼ˆæ¨¡æ‹Ÿï¼‰ï¼Œå¾—åˆ°$a_tâˆ¼ Ï€_t$ï¼Œ$a_t$å°±æ˜¯åŠ¨ä½œactionï¼Œ$Ï€_t$å°±æ˜¯æ‰€æœ‰åŠ¨ä½œçš„æ¦‚ç‡å‘é‡ã€‚æœ€ç»ˆåœ¨å±€é¢$s_T$çš„æ—¶å€™å¾—åˆ°èƒœå¹³è´Ÿçš„ç»“æœzï¼Œå°±æ˜¯æˆ‘ä»¬ä¸Šé¢æ‰€è¯´çš„valueã€‚\n",
    "\n",
    "According to a.Self-Play in Figure 1, the tree search (simulation) of the self-play from the situation $s_t$ will get $a_tâˆ¼ Ï€_t$, $a_t$ is the action action, and $Ï€_t$ is the probability of all actions vector. In the end, in the situation $s_T$, we get the result z, which is the value we mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è‡³æ­¤MCTSç®—æ³•å°±åˆ†æå®Œäº†ã€‚\n",
    "\n",
    "So far the MCTS algorithm has been analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç¥ç»ç½‘ç»œ Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, the MCTS is used to figure out which move to play. Then, after 1600 simulations, the next move is calculated, and the cycle continues until the winner is determined. In this way, the whole game is finished. This is a complete self-playing process. Then MCTS is equivalent to thinking in the brain. We save the position $s_t$ of each chess move, the calculated action probability vector $Ï€_t$ and the winning rate $z_t$ (that is, the return value), and use them as the game record data to train the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç¥ç»ç½‘ç»œçš„è¾“å…¥æ˜¯å±€é¢sï¼Œè¾“å‡ºæ˜¯é¢„æµ‹çš„actionæ¦‚ç‡å‘é‡på’Œèƒœç‡vï¼Œå…¬å¼ï¼š$(p,v)= f_{Î¸_i} (s)$ã€‚è®­ç»ƒç›®æ ‡æ˜¯æœ€å°åŒ–é¢„æµ‹èƒœç‡vå’Œè‡ªæˆ‘å¯¹å¼ˆçš„èƒœç‡zä¹‹é—´çš„è¯¯å·®ï¼Œå¹¶ä½¿ç¥ç»ç½‘ç»œèµ°å­æ¦‚ç‡pä¸æœç´¢æ¦‚ç‡Ï€çš„ç›¸ä¼¼åº¦æœ€å¤§åŒ–ã€‚æŒ‰ç…§è®ºæ–‡ä¸­æ‰€è¯´ï¼Œâ€œå…·ä½“è€Œè¨€ï¼Œå‚æ•°Î¸é€šè¿‡æ¢¯åº¦ä¸‹é™åˆ†åˆ«åœ¨å‡æ–¹è¯¯å·®å’Œäº¤å‰ç†µæŸå¤±ä¹‹å’Œä¸Šçš„æŸå¤±å‡½æ•°lè¿›è¡Œè°ƒæ•´ï¼Œ$l = (z - v)^2- Ï€^T  log p + c||Î¸||^2$ï¼Œå…¶ä¸­cæ˜¯æ§åˆ¶L2æƒé‡æ­£åˆ™åŒ–æ°´å¹³çš„å‚æ•°ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰ã€‚â€ç®€å•ç‚¹è¯´å°±æ˜¯è®©ç¥ç»ç½‘ç»œçš„é¢„æµ‹è·ŸMCTSçš„æœç´¢ç»“æœå°½é‡æ¥è¿‘ã€‚\n",
    "\n",
    "The input of the neural network is the position s, and the output is the predicted action probability vector p and the winning rate v. The formula: $(p,v) = f_{Î¸_i} (s)$. The training goal is to minimize the error between the predicted winning rate v and the winning rate z of the self-game, and to maximize the similarity between the neural network walk probability p and the search probability Ï€. According to the paper, \"Specifically, the parameter Î¸ is adjusted by the loss function l of the sum of the mean square error and the cross-entropy loss through gradient descent, $l = (z-v)^2- Ï€^T log p + c||Î¸||^2$, where c is the parameter that controls the L2 weight regularization level (to prevent overfitting).\" Simply put, the neural network prediction is as close as possible to the MCTS search result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "èƒœç‡æ˜¯å›å½’é—®é¢˜ï¼Œä¼˜åŒ–è‡ªç„¶ç”¨MSEæŸå¤±ï¼Œæ¦‚ç‡å‘é‡çš„ä¼˜åŒ–è¦ç”¨softmaxäº¤å‰ç†µæŸå¤±ï¼Œç›®æ ‡å°±æ˜¯æœ€å°åŒ–è¿™ä¸ªè”åˆæŸå¤±ã€‚\n",
    "\n",
    "The winning rate is a regression problem. The optimization naturally uses the MSE loss, and the optimization of the probability vector uses the softmax cross-entropy loss. The goal is to minimize this joint loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ç¥ç»ç½‘ç»œç»“æ„  Neural network structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç½‘ç»œç»“æ„æ²¡ä»€ä¹ˆå¥½è¯´çš„ï¼ŒæŒ‰ç…§è®ºæ–‡ä¸­çš„æè¿°å®ç°å³å¯ï¼Œä¸‹é¢æ˜¯ç»“æ„å›¾ï¼š\n",
    "\n",
    "There is nothing to say about the network structure, just follow the description in the paper to implement it. The following is the structure diagram:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![c2](./assets/c2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åˆ°æ­¤ï¼Œè¿™ç¯‡è®ºæ–‡åŸºæœ¬ä¸Šä»‹ç»çš„å·®ä¸å¤šäº†ï¼Œæœ‰äº›è®­ç»ƒå’Œä¼˜åŒ–æ–¹é¢çš„ç»†èŠ‚è¿™é‡Œå°±ä¸ä»‹ç»äº†ã€‚è¿‡ç¨‹å°±æ˜¯ç¥ç»ç½‘ç»œå…ˆéšæœºåˆå§‹åŒ–æƒé‡ï¼Œä½¿ç”¨MCTSä¸‹æ¯ä¸€æ­¥æ£‹ï¼Œå½“æ ‘ä¸­èŠ‚ç‚¹æ²¡æœ‰è¢«å±•å¼€æ—¶é€šè¿‡ç¥ç»ç½‘ç»œé¢„æµ‹å‡ºèµ°å­æ¦‚ç‡å’Œèƒœç‡æ·»åŠ åˆ°æ ‘ä¸Šï¼Œç„¶åä½¿ç”¨è‡ªæˆ‘å¯¹å¼ˆçš„æ•°æ®è®­ç»ƒç¥ç»ç½‘ç»œï¼Œåœ¨ä¸‹ä¸€æ¬¡è‡ªæˆ‘å¯¹å¼ˆä¸­ä½¿ç”¨æ–°çš„è®­ç»ƒè¿‡çš„ç¥ç»ç½‘ç»œè¿›è¡Œé¢„æµ‹ï¼ŒMCTSå’Œç¥ç»ç½‘ç»œä½ ä¸­æœ‰æˆ‘ã€æˆ‘ä¸­æœ‰ä½ ï¼Œå¦‚æ­¤åå¤è¿­ä»£ï¼Œç½‘ç»œé¢„æµ‹çš„æ›´å‡†ç¡®ï¼ŒMCTSçš„ç»“æœæ›´å¼ºå¤§ã€‚å®é™…ä¸Šç¥ç»ç½‘ç»œçš„é¢„æµ‹å¯ä»¥ç†è§£ä¸ºäººçš„ç›´è§‰ã€‚\n",
    "\n",
    "So far, this paper basically introduces almost the same, and some details of training and optimization are not introduced here. The process is that the neural network first randomly initializes the weights, uses MCTS to play each move, when the node in the tree is not expanded, the probability of move and the winning rate are predicted by the neural network and added to the tree, and then the neural network is trained using self-playing data. In the self-play game, a new trained neural network is used to make predictions. MCTS and neural networks have me in you and you in me. Repeated iterations will make the network predictions more accurate and the MCTS results more powerful. In fact, the prediction of neural network can be understood as human intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¥ä¸‹æ¥ä¸€èµ·çœ‹çœ‹AlphaZeroçš„è®ºæ–‡ã€‚\n",
    "\n",
    "Let's take a look at AlphaZero's paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç®—æ³•ä¸Šæ²¡æœ‰åŒºåˆ«ï¼Œåªæœ‰å‡ ä¸ªä¸åŒç‚¹ï¼š\n",
    "\n",
    "There is no difference in the algorithm, only a few differences:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - 1ã€åœ¨AlphaGo Zeroä¸­ï¼Œè‡ªæˆ‘å¯¹å¼ˆæ˜¯ç”±ä»¥å‰æ‰€æœ‰è¿­ä»£ä¸­æœ€å¥½çš„ç©å®¶ç”Ÿæˆçš„ã€‚æ¯æ¬¡è®­ç»ƒè¿­ä»£ä¹‹åï¼Œä¸æœ€å¥½ç©å®¶å¯¹å¼ˆæµ‹é‡æ–°ç©å®¶çš„èƒ½åŠ›ï¼›å¦‚æœä»¥55%çš„ä¼˜åŠ¿è·èƒœï¼Œé‚£ä¹ˆå®ƒå°†å–ä»£æœ€å¥½çš„ç©å®¶ï¼Œè€Œè‡ªæˆ‘å¯¹å¼ˆå°†ç”±è¿™ä¸ªæ–°ç©å®¶äº§ç”Ÿã€‚ç›¸åï¼ŒAlphaZeroåªç»´æŠ¤ä¸€ä¸ªä¸æ–­æ›´æ–°çš„å•ä¸ªç¥ç»ç½‘ç»œï¼Œè€Œä¸æ˜¯ç­‰å¾…è¿­ä»£å®Œæˆã€‚è‡ªæˆ‘å¯¹å¼ˆæ˜¯é€šè¿‡ä½¿ç”¨è¿™ä¸ªç¥ç»ç½‘ç»œçš„æœ€æ–°å‚æ•°ç”Ÿæˆçš„ï¼Œçœç•¥äº†è¯„ä¼°æ­¥éª¤å’Œé€‰æ‹©æœ€ä½³ç©å®¶çš„è¿‡ç¨‹ã€‚\n",
    " - 2ã€æ¯”èµ›ç»“æœé™¤äº†èƒœè´Ÿä»¥å¤–ï¼Œè¿˜æœ‰å¹³å±€ã€‚\n",
    " - 3ã€å›´æ£‹æ˜¯å¯ä»¥è¿›è¡Œæ•°æ®å¢å¼ºçš„ï¼Œå› ä¸ºå›´æ£‹çš„è§„åˆ™æ˜¯æ—‹è½¬å’Œåè½¬ä¸å˜çš„ã€‚ä½†æ˜¯è±¡æ£‹ã€å°†æ£‹ç­‰å°±ä¸è¡Œã€‚\n",
    " \n",
    " \n",
    " - 1. In AlphaGo Zero, self-play is generated by the best players in all previous iterations. After each training iteration, play against the best player to measure the ability of the new player; if you win with a 55% advantage, it will replace the best player, and the self-play will be generated by this new player. In contrast, AlphaZero only maintains a single neural network that is constantly updated, rather than waiting for the iteration to complete. The self game is generated by using the latest parameters of this neural network, omitting the evaluation step and the process of selecting the best player.\n",
    " - 2. In addition to the outcome of the game, there is a tie.\n",
    " - 3. Go can be enhanced by data, because the rules of Go are rotation and reversal unchanged. But chess, shogi, etc. cannot work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¥½åƒä¹Ÿæ²¡å•¥å¤§å˜åŒ–ï¼Œæˆ‘ä»¬é‡ç‚¹è¦è€ƒè™‘çš„æ˜¯è¾“å…¥ç‰¹å¾çš„è¡¨ç¤ºã€‚\n",
    "\n",
    "It doesn't seem to have changed much. The main thing we need to consider is the representation of the input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è¾“å…¥ç‰¹å¾çš„è¡¨ç¤º  Representation of input features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åˆšåˆšä»‹ç»ç¥ç»ç½‘ç»œçš„ç»“æ„æ—¶ï¼Œæ²¡æœ‰å¯¹è¾“å…¥ç‰¹å¾è¿›è¡Œè¯´æ˜ï¼Œå…ˆçœ‹çœ‹è®ºæ–‡ä¸­çš„å›¾ç¤ºã€‚\n",
    "\n",
    "When I just introduced the structure of the neural network, I didn't explain the input features. First look at the diagram in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![b6](./assets/b6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç½‘ç»œç»“æ„å›¾ä¸Šèƒ½å¤Ÿçœ‹å‡ºç¥ç»ç½‘ç»œçš„è¾“å…¥æ˜¯19Ã—19Ã—17ç»´åº¦çš„å›¾åƒæ ˆã€‚åŒ…å«17ä¸ªäºŒå€¼ï¼ˆåªæœ‰ä¸¤ä¸ªå€¼0å’Œ1ï¼‰ç‰¹å¾å¹³é¢ï¼Œ8ä¸ªç‰¹å¾å¹³é¢$X_t$ç”±äºŒè¿›åˆ¶å€¼ç»„æˆï¼Œè¡¨ç¤ºå½“å‰ç©å®¶å­˜åœ¨çš„æ£‹å­ï¼ˆå¦‚æœäº¤ç‚¹iåœ¨æ—¶é—´æ­¥tåŒ…å«ç©å®¶é¢œè‰²çš„æ£‹å­ï¼Œé‚£ä¹ˆ$X_t^i = 1$ï¼›å¦‚æœäº¤å‰ç‚¹æ˜¯ç©ºçš„ï¼ŒåŒ…å«å¯¹æ‰‹æ£‹å­ï¼Œæˆ–è€…t <0ï¼Œ$X_t^i  = 0$ï¼‰ã€‚å¦å¤–8ä¸ªç‰¹å¾å¹³é¢$Y_t$è¡¨ç¤ºå¯¹æ‰‹çš„æ£‹å­çš„ç›¸åº”ç‰¹å¾ã€‚ä¸ºä»€ä¹ˆæ¯ä¸ªç©å®¶8ä¸ªç‰¹å¾å¹³é¢å‘¢ï¼Ÿæ˜¯å› ä¸ºè¿™æ˜¯8æ­¥å†å²èµ°å­è®°å½•ï¼Œå°±æ˜¯è¯´æœ€è¿‘èµ°çš„8æ­¥æ£‹ä½œä¸ºè¾“å…¥ç‰¹å¾ã€‚æœ€åçš„ç‰¹å¾é¢Cè¡¨ç¤ºæ£‹å­é¢œè‰²ï¼ˆå½“å‰çš„æ£‹ç›˜çŠ¶æ€ï¼‰ï¼Œæ˜¯å¸¸é‡ï¼Œå¦‚æœæ˜¯é»‘è‰²æ£‹å­ï¼Œåˆ™ä¸º1ï¼Œå¦‚æœæ˜¯ç™½è‰²æ£‹å­åˆ™ä¸º0ã€‚è¿™äº›å¹³é¢è¿æ¥åœ¨ä¸€èµ·ï¼Œç»™å‡ºè¾“å…¥ç‰¹å¾$s_t$ = [$X_t , Y_t , X_{t-1}, Y_{t-1}, ..., X_{t-7}, Y_{t-7}, C$]ã€‚\n",
    "\n",
    "It can be seen from the network structure diagram that the input of the neural network is an image stack of 19Ã—19Ã—17 dimensions. Contains 17 binary (only two values â€‹â€‹0 and 1) feature planes, 8 feature planesğ‘‹ğ‘¡\n",
    "It is composed of binary values â€‹â€‹and represents the current player's pawn (if the intersection i contains the player's pawn at time step t, then ğ‘‹ğ‘–ğ‘¡=1; if the intersection is empty and contains the opponent's pawn, or t <0, ğ‘‹ğ‘–=0) . The other 8 feature planes ğ‘Œğ‘¡ represent the corresponding features of the opponent's pieces. Why are there 8 feature planes for each player? It is because this is a record of 8-step historical moves, that is, the most recent 8 moves are used as input features. The last characteristic surface C represents the color of the chess piece (the current state of the board), which is a constant. If it is a black piece, it is 1, and if it is a white piece, it is 0. These planes are connected together to give the input feature ğ‘ ğ‘¡ = [ğ‘‹ğ‘¡,ğ‘Œğ‘¡,ğ‘‹ğ‘¡âˆ’1,ğ‘Œğ‘¡âˆ’1,...,ğ‘‹ğ‘¡âˆ’7,ğ‘Œğ‘¡âˆ’7,ğ¶].\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å›½é™…è±¡æ£‹å°±ä¸åŒäº†ï¼ŒåŠ å…¥äº†å„ç§ç‰¹å¾å¹³é¢ï¼Œç”¨æ¥è¡¨ç¤ºä¸åŒçš„æƒ…å†µï¼Œç‹è½¦æ˜“ä½å•¦ï¼Œå¤šå°‘å›åˆæ²¡æœ‰è¿›å±•å•¦ï¼ˆæ²¡æœ‰åƒå­ï¼‰ï¼Œé‡å¤çš„å±€é¢å•¦ï¼ˆå¤šæ¬¡é‡å¤ä¼šè¢«åˆ¤å¹³å±€ï¼‰ç­‰ç­‰ï¼Œè¿™äº›ä¸æ˜¯æˆ‘æƒ³è¯´çš„ï¼Œè¿™äº›ç‰¹å¾å¯ä»¥æ ¹æ®ä¸åŒçš„æ£‹ç§è‡ªå·±å»è®¾è®¡ï¼Œæˆ‘ä»¬é‡ç‚¹å…³æ³¨çš„æ˜¯æ£‹å­çš„ç‰¹å¾ã€‚\n",
    "\n",
    "Chess is different. Various characteristic planes are added to indicate different situations. The king and the rook are transposed, how many rounds have not been progressed (there is no game), and the position is repeated (multiple repetitions will be considered a tie). Wait, these are not what I want to say, these features can be designed according to different chess types, we focus on the characteristics of chess pieces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¯¹äºå›´æ£‹è€Œè¨€ï¼Œæ¯ä¸ªæ£‹å­éƒ½æ˜¯ä¸€æ ·çš„ï¼Œéƒ½æ˜¯ä¸€ç±»ã€‚è€Œå›½é™…è±¡æ£‹åˆ†ä¸º6ç§æ£‹å­ï¼šè½¦ã€é©¬ã€è±¡ã€åã€ç‹ã€å…µï¼Œé‚£åœ¨ç‰¹å¾å¹³é¢ä¸Šæ€ä¹ˆè¡¨ç¤ºå‘¢ï¼Œæ€»ä¸èƒ½ä½¿ç”¨0~5å§ã€‚è¿˜æ˜¯ç”¨0å’Œ1æ¥è¡¨ç¤ºæ£‹ç›˜ä¸Šæœ‰å­è¿˜æ˜¯æ²¡å­ï¼Œç„¶åæ—¢ç„¶æ˜¯6ç±»æ£‹å­ï¼Œæƒ³å½“ç„¶çš„ä½¿ç”¨one-hotç¼–ç äº†ï¼Œæ‰€ä»¥ç‰¹å¾å¹³é¢åˆ†æˆäº†6ä¸ªå¹³é¢ï¼Œæ¯ä¸€ä¸ªå¹³é¢ç”¨æ¥è¡¨ç¤ºä¸åŒç§ç±»æ£‹å­åœ¨æ£‹ç›˜ä¸Šçš„ä½ç½®ã€‚\n",
    "\n",
    "For Go, every chess piece is the same and all of the same type. There are 6 types of chess pieces: rook, horse, bishop, queen, king, and pawn. How do you express it on the characteristic plane? You can't use 0~5. Still use 0 and 1 to indicate whether there are stones or no stones on the chessboard. Then since it is a 6 types of chess pieces, one-hot encoding is taken for granted, so the feature plane is divided into 6 planes, and each plane is used to represent different types of chess pieces. The position on the chessboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»¥ä¸Šå°±æ˜¯ä»‹ç»çš„å…¨éƒ¨äº†ï¼Œæ›´å¤šçš„ç»†èŠ‚ï¼Œæ¯”å¦‚ä¼˜åŒ–å‚æ•°è®¾ä¸ºå¤šå°‘ã€å­¦ä¹ ç‡é€€ç«è®¾ä¸ºå¤šå°‘ç­‰ç­‰è¯·é˜…è¯»è®ºæ–‡ã€‚\n",
    "\n",
    "The above is all that was introduced. For more details, such as how many optimization parameters are set, how much learning rate annealing is set, etc., please read the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¸­å›½è±¡æ£‹çš„å®ç°  The realization of Chinese chess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åŸç†è®²äº†ä¸€å¤§å †ï¼Œè¯¥ä¸Šä»£ç äº†ï¼Œè¿™é‡Œæ ¹æ®è®ºæ–‡ä¸­çš„ç®—æ³•å®ç°ä¸€ä¸ªä¸­å›½è±¡æ£‹ç¨‹åºã€‚\n",
    "\n",
    "A lot of principles have been discussed, it is time to code, here is a Chinese chess program based on the algorithm in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å®Œæ•´ä»£ç è¯·å‚è§[`é¡¹ç›®åœ°å€`](https://github.com/chengstone/cchess-zero)\n",
    "\n",
    "For the complete code, please see [`Project Address`](https://github.com/chengstone/cchess-zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è¾“å…¥ç‰¹å¾çš„è®¾è®¡  Design of input features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å…ˆå®ç°ç¥ç»ç½‘ç»œçš„éƒ¨åˆ†ï¼Œé‚£ä¹ˆå°±è¦å…ˆè®¾è®¡è¾“å…¥ç‰¹å¾ã€‚å…¶å®è·Ÿå›½é™…è±¡æ£‹å·®ä¸å¤šï¼Œæ£‹å­åˆ†ä¸ºï¼šè½¦ã€é©¬ã€ç‚®ã€è±¡ã€å£«ã€å°†ã€å…µï¼Œå…±7ç§æ£‹å­ï¼Œé‚£å°±æ˜¯æ¯ä¸ªç©å®¶7ä¸ªç‰¹å¾å¹³é¢ï¼Œä¸€å…±14ä¸ªç‰¹å¾å¹³é¢ã€‚è‡³äºè®ºæ–‡ä¸­å…¶ä»–çš„ç‰¹å¾å¹³é¢ï¼Œæ¯”å¦‚é¢œè‰²ã€å›åˆæ•°ã€é‡å¤å±€é¢ã€å†å²èµ°å­è®°å½•ç­‰ç­‰æˆ‘æ²¡æœ‰å®ç°ï¼Œåªä½¿ç”¨äº†å½“å‰æ£‹ç›˜ä¸Šæ¯ä¸ªç©å®¶æ¯ä¸ªæ£‹å­çš„ä½ç½®ç‰¹å¾ä½œä¸ºè¾“å…¥ï¼Œä¸€å…±14ä¸ªå¹³é¢ï¼Œå½“ç„¶è®ºæ–‡ä¸­è¯´çš„å…¶ä»–ç‰¹å¾å¹³é¢æ‚¨ä¹Ÿå¯ä»¥å®ç°ä¸€ä¸‹è¯•è¯•ã€‚æ£‹ç›˜å¤§å°æ˜¯$9 * 10$ï¼Œæ‰€ä»¥è¾“å…¥å ä½ç¬¦å°±æ˜¯ï¼š\n",
    "\n",
    "To implement the neural network first, then the input features must be designed first. In fact, similar to chess, chess pieces are divided into: rook, horse, artillery, bishop, soldier, general, and pawn. There are 7 types of chess pieces, that is, each player has 7 characteristic planes, and a total of 14 characteristic planes. As for the other feature planes in the paper, such as color, number of rounds, repeated positions, historical move records, etc., I did not implement it. I only used the position features of each player and each piece on the current board as input, a total of 14 planes, Of course, you can also try the other feature planes mentioned in the paper. The board size is $9 * 10$, so the input placeholder is: input feature design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "self.inputs_ = tf.placeholder(tf.float32, [None, 9, 10, 14], name='inputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![c3](./assets/c3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¥ä¸‹æ¥æ˜¯å®šä¹‰è¾“å…¥çš„æ¦‚ç‡å‘é‡piï¼ˆÏ€ï¼‰ï¼Œéœ€è¦ç¡®å®šå‘é‡çš„é•¿åº¦ï¼Œæ„å‘³ç€éœ€è¦ç¡®å®šæ‰€æœ‰åˆæ³•èµ°å­çš„é›†åˆé•¿åº¦ã€‚å‡½æ•°å¦‚ä¸‹ï¼š\n",
    "\n",
    "The next step is to define the input probability vector pi (Ï€). The length of the vector needs to be determined, which means that the set length of all legal walkers needs to be determined. The function is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºæ‰€æœ‰åˆæ³•èµ°å­UCIï¼Œsize 2086\n",
    "# Create all legal pawns UCI, size 2086\n",
    "def create_uci_labels():\n",
    "    labels_array = []\n",
    "    letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i']\n",
    "    numbers = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "    Advisor_labels = ['d7e8', 'e8d7', 'e8f9', 'f9e8', 'd0e1', 'e1d0', 'e1f2', 'f2e1',\n",
    "                      'd2e1', 'e1d2', 'e1f0', 'f0e1', 'd9e8', 'e8d9', 'e8f7', 'f7e8']\n",
    "    Bishop_labels = ['a2c4', 'c4a2', 'c0e2', 'e2c0', 'e2g4', 'g4e2', 'g0i2', 'i2g0',\n",
    "                     'a7c9', 'c9a7', 'c5e7', 'e7c5', 'e7g9', 'g9e7', 'g5i7', 'i7g5',\n",
    "                     'a2c0', 'c0a2', 'c4e2', 'e2c4', 'e2g0', 'g0e2', 'g4i2', 'i2g4',\n",
    "                     'a7c5', 'c5a7', 'c9e7', 'e7c9', 'e7g5', 'g5e7', 'g9i7', 'i7g9']\n",
    "\n",
    "    for l1 in range(9):\n",
    "        for n1 in range(10):\n",
    "            destinations = [(t, n1) for t in range(9)] + \\\n",
    "                           [(l1, t) for t in range(10)] + \\\n",
    "                           [(l1 + a, n1 + b) for (a, b) in\n",
    "                            [(-2, -1), (-1, -2), (-2, 1), (1, -2), (2, -1), (-1, 2), (2, 1), (1, 2)]]  # é©¬èµ°æ—¥\n",
    "            for (l2, n2) in destinations:\n",
    "                if (l1, n1) != (l2, n2) and l2 in range(9) and n2 in range(10):\n",
    "                    move = letters[l1] + numbers[n1] + letters[l2] + numbers[n2]\n",
    "                    labels_array.append(move)\n",
    "\n",
    "    for p in Advisor_labels:\n",
    "        labels_array.append(p)\n",
    "\n",
    "    for p in Bishop_labels:\n",
    "        labels_array.append(p)\n",
    "\n",
    "    return labels_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é•¿åº¦ä¸€å…±æ˜¯2086ã€‚å…³äºUCCIçš„èµ„æ–™å¯ä»¥å‚è€ƒï¼š[`ä¸­å›½è±¡æ£‹é€šç”¨å¼•æ“åè®®ã€€ç‰ˆæœ¬ï¼š3.0`](http://www.xqbase.com/protocol/cchess_ucci.htm)\n",
    "\n",
    "The total length is 2086. For information about UCCI, please refer to: [`Chinese Chess General Engine Protocolã€€Version: 3.0`](http://www.xqbase.com/protocol/cchess_ucci.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¦‚ç‡å‘é‡piçš„å ä½ç¬¦å®šä¹‰ï¼šself.pi_ = tf.placeholder(tf.float32, [None, 2086], name='pi')\n",
    "\n",
    "The placeholder definition of the probability vector pi: self.pi_ = tf.placeholder(tf.float32, [None, 2086], name='pi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "èƒœç‡zçš„å ä½ç¬¦å®šä¹‰ï¼šself.z_ = tf.placeholder(tf.float32, [None, 1], name='z')\n",
    "\n",
    "The placeholder definition of winning percentage z: self.z_ = tf.placeholder(tf.float32, [None, 1], name='z')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å­¦ä¹ ç‡çš„å®šä¹‰ï¼šself.learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "Definition of learning rate: self.learning_rate = tf.placeholder(tf.float32, name='learning_rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¼˜åŒ–å™¨ä½¿ç”¨Momentumï¼š    The optimizer uses Momentum:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "self.momentum = 0.9\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=self.learning_rate, momentum=self.momentum, use_nesterov=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™é‡Œéœ€è¦ç‰¹æ®Šè¯´æ˜ä¸€ä¸‹ï¼Œæˆ‘å®ç°çš„æ˜¯å¤šGPUè®­ç»ƒï¼Œå…³äºå¤šGPUè®­ç»ƒç¥ç»ç½‘ç»œçš„å®ç°å¯ä»¥å‚è€ƒTensorFlowå®˜æ–¹çš„[`ä¾‹å­`](https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py)ï¼Œå’Œ[`TensorFlowå¤šGPUå¹¶è¡Œè®¡ç®—å®ä¾‹---MNIST`](https://gitee.com/liyang619/mnist_multi_gpu_batching_train/blob/master/mnist_multi_gpu_batching_train.py)ã€‚\n",
    "\n",
    "Special explanation is needed here. I have implemented multi-GPU training. For the implementation of multi-GPU training neural network, please refer to the official [TensorFlow example](https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py) and the TensorFlow multi-GPU parallel computing example [MNIST](https://gitee.com/liyang619/mnist_multi_gpu_batching_train/blob/master/mnist_multi_gpu_batching_train.py)ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å®ç°æ€æƒ³æ˜¯æŠŠè¾“å…¥æ•°æ®æŒ‰ç…§ä½¿ç”¨çš„gpuæ•°é‡å‡åˆ†ï¼š\n",
    "\n",
    "The realization idea is to divide the input data equally according to the number of GPUs used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inputs_batches = tf.split(self.inputs_, self.num_gpus, axis=0)\n",
    "pi_batches = tf.split(self.pi_, self.num_gpus, axis=0)\n",
    "z_batches = tf.split(self.z_, self.num_gpus, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \"\"\"COMMENTS ON NEXT SEGMENT OF CODE\"\"\"\n",
    "    tower_grads = [None] * self.num_gpus\n",
    "\n",
    "    self.loss = 0\n",
    "    self.accuracy = 0\n",
    "    self.policy_head = []\n",
    "    self.value_head = []\n",
    "\n",
    "    with tf.variable_scope(tf.get_variable_scope()):\n",
    "        # Build the core model within the graph.\n",
    "        for i in range(self.num_gpus):   \n",
    "        # ä¸åŒçš„gpuåˆ†åˆ«ä½¿ç”¨ä¸åŒçš„name scope  # Different GPUs use different name scopes\n",
    "        with tf.device('/gpu:%d' % i):   \n",
    "            with tf.name_scope('TOWER_{}'.format(i)) as scope: \n",
    "                    # å°†ä¸Šé¢å‡åˆ†çš„è¾“å…¥æ•°æ®è¾“å…¥ç»™å„è‡ªçš„gpuè¿›è¡Œè¿ç®—\n",
    "                    # Input the input data equally divided into the respective gpu for calculation\n",
    "                inputs_batch, pi_batch, z_batch = inputs_batches[i], pi_batches[i], z_batches[i]\n",
    "                    # åˆ’é‡ç‚¹ï¼è¿ç®—å›¾çš„æ„å»ºä¸€å®šè¦å•ç‹¬å†™åœ¨æ–°çš„å‡½æ•°ä¸­ï¼Œè¿™æ ·è¿è¡Œæ‰ä¸ä¼šå‡ºé”™ï¼Œå¦åˆ™TensorFlowä¼šæç¤ºä¸èƒ½é‡å¤ä½¿ç”¨å˜é‡ã€‚**\n",
    "                loss = self.tower_loss(inputs_batch, pi_batch, z_batch, i) # æ„å»ºç¥ç»ç½‘ç»œè®¡ç®—å›¾çš„å‡½æ•°ï¼Œä¸€ä¼šè¯¦ç»†è¯´ã€‚\n",
    "                    # Key points! The construction of the arithmetic graph must be written separately in the new function, so that the operation will not go wrong, otherwise TensorFlow will prompt that the variable cannot be reused. loss = self.tower_loss(inputs_batch, pi_batch, z_batch, i) # The function of constructing the neural network calculation graph, I will talk about it in detail later.\n",
    "                    # reuse variable happens here\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "                grad = optimizer.compute_gradients(loss)\n",
    "                tower_grads[i] = grad    # ä¿å­˜æ¯ä¸€ä¸ªgpuçš„æ¢¯åº¦  # Save the gradient of each gpu\n",
    "    # lossæ˜¯å¤šä¸ªgpuçš„lossæ€»å’Œï¼Œæ‰€ä»¥è¦å–å¹³å‡  # Loss is the sum of the loss of multiple GPUs, so take the average\n",
    "    self.loss /= self.num_gpus    \n",
    "    self.accuracy /= self.num_gpus    # accä¹Ÿæ˜¯åŒç†  # acc is the same\n",
    "    grads = self.average_gradients(tower_grads) # åŒç†ï¼Œå¯¹æ‰€æœ‰æ¢¯åº¦å–å¹³å‡  # Similarly, take the average of all gradients\n",
    "    self.train_op = optimizer.apply_gradients(grads, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å®ç°ç¥ç»ç½‘ç»œè®¡ç®—å›¾  Implement neural network computational graph convolution block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™é‡Œå®Œå…¨æ˜¯æŒ‰ç…§è®ºæ–‡æ‰€è¿°çš„ç¥ç»ç½‘ç»œç»“æ„å®ç°çš„ï¼Œå¤§å®¶å¯ä»¥å¯¹ç…§ä¸Šé¢çš„ç»“æ„å›¾ï¼Œæ˜¯ä¸€ä¸€å¯¹åº”çš„ã€‚ç¨æœ‰ä¸åŒçš„æ˜¯ï¼Œfilters sizeæˆ‘è®¾ä¸º128ï¼Œæ²¡æœ‰ä½¿ç”¨256ã€‚å¦å¤–æ®‹å·®å—çš„æ•°é‡æˆ‘é»˜è®¤ä½¿ç”¨äº†7å±‚ï¼Œæ²¡æœ‰ä½¿ç”¨19æˆ–è€…39ï¼Œå¤§å®¶ç”µè„‘ç»™åŠ›çš„è¯å¯ä»¥å°è¯•ä¿®æ”¹ä¸€ä¸‹ã€‚\n",
    "\n",
    "This is completely implemented in accordance with the neural network structure described in the paper. You can compare the structure diagram above, which is a one-to-one correspondence. The slight difference is that I set the filters size to 128 instead of 256. In addition, I used 7 layers by default for the number of residual blocks, and did not use 19 or 39. You can try to modify it if your computer is strong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tower_loss(self, inputs_batch, pi_batch, z_batch, i):\n",
    "    # å·ç§¯å— # Convolution block\n",
    "    with tf.variable_scope('init'):\n",
    "        layer = tf.layers.conv2d(inputs_batch, self.filters_size, 3, padding='SAME')  # filters 128(or 256)\n",
    "\n",
    "        layer = tf.contrib.layers.batch_norm(layer, center=False, epsilon=1e-5, fused=True,\n",
    "                                            is_training=self.training, activation_fn=tf.nn.relu)    # epsilon = 0.25\n",
    "\n",
    "    # æ®‹å·®å—  # Residual block\n",
    "    with tf.variable_scope(\"residual_block\"):\n",
    "        for _ in range(self.res_block_nums):\n",
    "            layer = self.residual_block(layer)\n",
    "\n",
    "    # ç­–ç•¥å¤´ # Strategy header\n",
    "    with tf.variable_scope(\"policy_head\"):\n",
    "        policy_head = tf.layers.conv2d(layer, 2, 1, padding='SAME')\n",
    "        policy_head = tf.contrib.layers.batch_norm(policy_head, center=False, epsilon=1e-5, fused=True,\n",
    "                                                    is_training=self.training, activation_fn=tf.nn.relu)\n",
    "\n",
    "        # print(self.policy_head.shape)  # (?, 9, 10, 2)\n",
    "        policy_head = tf.reshape(policy_head, [-1, 9 * 10 * 2])\n",
    "        policy_head = tf.contrib.layers.fully_connected(policy_head, self.prob_size, activation_fn=None)\n",
    "        self.policy_head.append(policy_head)    # ä¿å­˜å¤šä¸ªgpuçš„ç­–ç•¥å¤´ç»“æœï¼ˆèµ°å­æ¦‚ç‡å‘é‡ï¼‰# Save the strategy header results of multiple GPUs \n",
    "\n",
    "    # ä»·å€¼å¤´ # Value Head\n",
    "    with tf.variable_scope(\"value_head\"):\n",
    "        value_head = tf.layers.conv2d(layer, 1, 1, padding='SAME')\n",
    "        value_head = tf.contrib.layers.batch_norm(value_head, center=False, epsilon=1e-5, fused=True,\n",
    "                                        is_training=self.training, activation_fn=tf.nn.relu)\n",
    "        # print(self.value_head.shape)  # (?, 9, 10, 1)\n",
    "        value_head = tf.reshape(value_head, [-1, 9 * 10 * 1])\n",
    "        value_head = tf.contrib.layers.fully_connected(value_head, 256, activation_fn=tf.nn.relu)\n",
    "        value_head = tf.contrib.layers.fully_connected(value_head, 1, activation_fn=tf.nn.tanh)\n",
    "        self.value_head.append(value_head)    # ä¿å­˜å¤šä¸ªgpuçš„ä»·å€¼å¤´ç»“æœï¼ˆèƒœç‡ï¼‰ #Save header results\n",
    "\n",
    "    # æŸå¤±  # Loss\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        policy_loss = tf.nn.softmax_cross_entropy_with_logits(labels=pi_batch, logits=policy_head)    \n",
    "        policy_loss = tf.reduce_mean(policy_loss)\n",
    "\n",
    "        # value_loss = tf.squared_difference(z_batch, value_head)\n",
    "        value_loss = tf.losses.mean_squared_error(labels=z_batch, predictions=value_head)    \n",
    "        value_loss = tf.reduce_mean(value_loss)\n",
    "        tf.summary.scalar('mse_tower_{}'.format(i), value_loss)\n",
    "\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(scale=self.c_l2)\n",
    "        regular_variables = tf.trainable_variables()\n",
    "        l2_loss = tf.contrib.layers.apply_regularization(regularizer, regular_variables)\n",
    "\n",
    "        # loss = value_loss - policy_loss + l2_loss\n",
    "        loss = value_loss + policy_loss + l2_loss    # softmaxäº¤å‰ç†µæŸå¤± + MSE + l2æŸå¤± # # softmax cross entropy loss + MSE + l2 loss\n",
    "        self.loss += loss                            # å¤šä¸ªgpuçš„lossæ€»å’Œ  # The sum of loss of multiple GPUs\n",
    "        tf.summary.scalar('loss_tower_{}'.format(i), loss)\n",
    "\n",
    "    with tf.variable_scope(\"accuracy\"):\n",
    "        # Accuracy    è¿™ä¸ªå‡†ç¡®ç‡æ˜¯é¢„æµ‹æ¦‚ç‡å‘é‡å’ŒMCTSçš„æ¦‚ç‡å‘é‡çš„æ¯”è¾ƒ\n",
    "        # This accuracy is the comparison between the predicted probability vector and the MCTS probability vector\n",
    "        correct_prediction = tf.equal(tf.argmax(policy_head, 1), tf.argmax(pi_batch, 1))    \n",
    "        correct_prediction = tf.cast(correct_prediction, tf.float32)\n",
    "        accuracy = tf.reduce_mean(correct_prediction, name='accuracy')\n",
    "        self.accuracy += accuracy\n",
    "        tf.summary.scalar('move_accuracy_tower_{}'.format(i), accuracy)\n",
    "    return loss\n",
    "\n",
    "def residual_block(self, in_layer):\n",
    "    orig = tf.identity(in_layer)\n",
    "\n",
    "    layer = tf.layers.conv2d(in_layer, self.filters_size, 3, padding='SAME')  # filters 128(or 256)\n",
    "    layer = tf.contrib.layers.batch_norm(layer, center=False, epsilon=1e-5, fused=True,\n",
    "                                        is_training=self.training, activation_fn=tf.nn.relu)\n",
    "\n",
    "    layer = tf.layers.conv2d(layer, self.filters_size, 3, padding='SAME')  # filters 128(or 256)\n",
    "    layer = tf.contrib.layers.batch_norm(layer, center=False, epsilon=1e-5, fused=True, is_training=self.training)\n",
    "    out = tf.nn.relu(tf.add(orig, layer))\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è®­ç»ƒç½‘ç»œ  Training Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(self, positions, probs, winners, learning_rate):\n",
    "    feed_dict = {\n",
    "        self.inputs_: positions,\n",
    "        self.training: True,\n",
    "        self.learning_rate: learning_rate,\n",
    "        self.pi_: probs,\n",
    "        self.z_: winners\n",
    "    }\n",
    "        \n",
    "    _, accuracy, loss, global_step, summary = self.sess.run([self.train_op, self.accuracy, self.loss, self.global_step, self.summaries_op], feed_dict=feed_dict)\n",
    "    self.train_writer.add_summary(summary, global_step)\n",
    "\n",
    "    return accuracy, loss, global_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä½¿ç”¨ç¥ç»ç½‘ç»œé¢„æµ‹  Use neural network predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é¢„æµ‹çš„ä»£ç ç¨å¾®éº»çƒ¦ä¸€äº›ï¼Œå› ä¸ºæˆ‘ä»¬è‡ªå¯¹å¼ˆè®­ç»ƒæ—¶æ˜¯å¤šçº¿ç¨‹åœ¨è·‘çš„ï¼Œä¼ è¿‡æ¥çš„è¾“å…¥æ•°æ®å¯èƒ½å¹¶ä¸èƒ½è¢«gpuæ•°é‡å‡åˆ†ï¼Œæ¯”å¦‚æˆ‘æœ‰2ä¸ªgpuï¼Œä½†æ˜¯ä¼ è¿›æ¥çš„è¾“å…¥sizeæ˜¯3ï¼Œè¿™æ ·çš„è¯å°±æœ‰ä¸€ä¸ªgpuè·‘2ä¸ªæ•°æ®ï¼Œä¸€ä¸ªgpuè·‘1ä¸ªæ•°æ®ã€‚å¯å®é™…ä¸Šè¿™æ ·ä»£ç æ˜¯è·‘ä¸èµ·æ¥çš„ï¼Œä¼šæŠ¥é”™ï¼Œæˆ‘googleäº†åŠå¤©ä¹Ÿæ²¡æ‰¾åˆ°è§£å†³åŠæ³•ã€‚\n",
    "\n",
    "The prediction code is a bit more troublesome, because we are running in multiple threads during self-game training, and the input data passed over may not be evenly divided by the number of GPUs. For example, I have 2 GPUs, but the input size passed in is 3. In this case, one gpu runs 2 data, and one gpu runs 1 data. But in fact, this code can't run, and it will report an error. I googled for a long time and couldn't find a solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘çš„è§£å†³æ–¹æ¡ˆæ˜¯ï¼Œå…ˆçœ‹çœ‹è¾“å…¥æ•°æ®çš„é•¿åº¦èƒ½å¦è¢«gpuæ•°é‡æ•´é™¤ï¼Œå¦‚æœèƒ½ï¼Œé‚£å°±ä¸€åˆ‡æ­£å¸¸ï¼Œç›´æ¥æŠŠè¾“å…¥ä¼ ç»™ç½‘ç»œå°±å¥½ï¼Œç¥ç»ç½‘ç»œä¼šå°†æ•°æ®æŒ‰ç…§gpuæ•°é‡å‡åˆ†ã€‚\n",
    "\n",
    "My solution is to first see if the length of the input data can be divisible by the number of GPUs. If so, everything is normal. Just pass the input directly to the network. The neural network will divide the data equally by the number of GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸€æ—¦ä¸èƒ½æ•´é™¤ï¼Œé‚£å°±æŠŠè¾“å…¥æ•°æ®åˆ†æˆä¸¤éƒ¨åˆ†ï¼Œä¸€éƒ¨åˆ†æ˜¯èƒ½è¢«gpuæ•°é‡æ•´é™¤çš„æ•°æ®ï¼Œä¸€éƒ¨åˆ†æ˜¯ä½™ä¸‹çš„æ•°æ®ã€‚æ¯”å¦‚æˆ‘æœ‰2ä¸ªgpuï¼Œè¾“å…¥æ•°æ®çš„é•¿åº¦æ˜¯5ï¼Œé‚£ä¹ˆæŠŠè¿™5ä»½æ•°æ®åˆ†æˆ4ä»½å’Œ1ä»½ã€‚4ä»½æ•°æ®çš„å¤„ç†å°±æ˜¯æ­£å¸¸å¤„ç†ï¼Œç›´æ¥æŠŠæ•°æ®ä¼ ç»™ç½‘ç»œå°±å¥½ï¼Œç¥ç»ç½‘ç»œä¼šå°†æ•°æ®æŒ‰ç…§gpuæ•°é‡å‡åˆ†ã€‚\n",
    "\n",
    "Once it is not divisible, divide the input data into two parts, one is the data that can be divisible by the number of GPUs, and the other is the remaining data. For example, if I have 2 GPUs and the length of the input data is 5, then the 5 pieces of data will be divided into 4 parts and 1 part. The processing of 4 pieces of data is normal processing, just send the data directly to the network, and the neural network will divide the data equally according to the number of GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½™ä¸‹çš„é‚£éƒ¨åˆ†æ•°æ®æ€ä¹ˆå¤„ç†å‘¢ï¼ŸæŠŠä½™ä¸‹çš„æ•°æ®ä¸æ–­å †å èµ·æ¥ï¼Œç›´åˆ°æ•°æ®èƒ½å¤Ÿè¢«gpuæ•°é‡å‡åˆ†ä¸ºæ­¢ã€‚å‡å¦‚å‰©ä¸‹1ä»½æ•°æ®ï¼Œé‚£å°±å¤åˆ¶1ä»½ï¼Œå˜æˆ2ä»½ç›¸åŒçš„æ•°æ®ï¼Œè¿™æ ·æ­£å¥½è¢«2ä¸ªgpuæ•°é‡å‡åˆ†ã€‚åªä¸è¿‡è¿™2ä¸ªgpuå¤„ç†åè¿”å›çš„æ•°æ®ï¼Œæˆ‘ä»¬åªè¦ä¸€ä¸ªgpuçš„ç»“æœå°±è¡Œäº†ï¼ŒæŠ›å¼ƒå¦å¤–ä¸€ä¸ªã€‚\n",
    "\n",
    "How to deal with the rest of the data? Stack the remaining data until the data can be divided equally by the number of GPUs. If there is 1 copy of data left, then copy 1 copy and become 2 copies of the same data, so that it is equally divided by the number of 2 GPUs. It's just that the data returned after these 2 GPUs are processed, we only need the result of one GPU, and discard the other one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™æ®µä»£ç æˆ‘åªåœ¨awsçš„2ä¸ªgpuçš„ç¯å¢ƒä¸‹è·‘è¿‡ï¼Œæ›´å¤šçš„gpuå°±æ²¡è¯•è¿‡äº†ï¼Œä¹Ÿè®¸æœ‰bugä¹Ÿä¸ä¸€å®šï¼Œæ‚¨å¯ä»¥è·‘è·‘çœ‹ï¼šï¼‰\n",
    "\n",
    "I have only ran this code in the environment of 2 GPUs in aws. I havenâ€™t tried more GPUs. Maybe there are bugs. You can run and see:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@profile\n",
    "def forward(self, positions):  # , probs, winners\n",
    "    # print(\"positions.shape : \", positions.shape)\n",
    "    positions = np.array(positions)\n",
    "    batch_n = positions.shape[0] // self.num_gpus\n",
    "    alone = positions.shape[0] % self.num_gpus\n",
    "\n",
    "    if alone != 0:    # åˆ¤æ–­æ˜¯å¦ä¸èƒ½è¢«gpuå‡åˆ† # Determine whether it cannot be divided equally by gpu\n",
    "        if(positions.shape[0] != 1):  # å¦‚æœä¸æ­¢1ä»½æ•°æ®ã€‚å› ä¸ºæœ‰å¯èƒ½è¾“å…¥æ•°æ®çš„é•¿åº¦æ˜¯1ï¼Œè¿™æ ·è‚¯å®šä¸èƒ½è¢«å¤šgpuå‡åˆ†äº†ã€‚\n",
    "                                    # If there is more than one piece of data. Because it is possible that the length of the input data is 1, it must not be evenly divided by multiple GPUs.\n",
    "            feed_dict = {\n",
    "                self.inputs_: positions[:positions.shape[0] - alone],    # å…ˆå°†èƒ½å‡åˆ†çš„è¿™éƒ¨åˆ†æ•°æ®ä¼ å…¥ç¥ç»ç½‘ç»œ # First pass this part of the data that can be divided into the neural network\n",
    "                self.training: False\n",
    "            }\n",
    "            action_probs, value = self.sess.run([self.policy_head, self.value_head], feed_dict=feed_dict)\n",
    "            action_probs, value = np.vstack(action_probs), np.vstack(value)\n",
    "\n",
    "        new_positions = positions[positions.shape[0] - alone:]    # å–ä½™ä¸‹çš„è¿™éƒ¨åˆ†æ•°æ®  # Take the remaining part of the data\n",
    "        pos_lst = []\n",
    "        while len(pos_lst) == 0 or (np.array(pos_lst).shape[0] * np.array(pos_lst).shape[1]) % self.num_gpus != 0:\n",
    "            pos_lst.append(new_positions)    # å°†ä½™ä¸‹çš„è¿™éƒ¨åˆ†æ•°æ®å †å èµ·æ¥ï¼Œç›´åˆ°æ•°é‡çš„é•¿åº¦èƒ½è¢«gpuå‡åˆ†  # Stack the remaining part of the data until the length of the quantity can be divided equally by the gpu\n",
    "\n",
    "        if(len(pos_lst) != 0):\n",
    "            shape = np.array(pos_lst).shape\n",
    "            pos_lst = np.array(pos_lst).reshape([shape[0] * shape[1], 9, 10, 14])\n",
    "            \n",
    "        # å°†æ•°æ®ä¼ å…¥ç½‘ç»œï¼Œå¾—åˆ°ä¸èƒ½è¢«gpuå‡åˆ†çš„æ•°æ®çš„è®¡ç®—ç»“æœ  # Pass the data to the network and get the calculation result of the data that cannot be divided equally by the GPU\n",
    "        feed_dict = {\n",
    "            self.inputs_: pos_lst,\n",
    "            self.training: False\n",
    "        }\n",
    "        action_probs_2, value_2 = self.sess.run([self.policy_head, self.value_head], feed_dict=feed_dict)\n",
    "            # print(\"action_probs_2.shape : \", np.array(action_probs_2).shape)\n",
    "            # print(\"value_2.shape : \", np.array(value_2).shape)\n",
    "        action_probs_2, value_2 = action_probs_2[0], value_2[0]\n",
    "            # print(\"------------------------\")\n",
    "            # print(\"action_probs_2.shape : \", np.array(action_probs_2).shape)\n",
    "            # print(\"value_2.shape : \", np.array(value_2).shape)\n",
    "\n",
    "        if(positions.shape[0] != 1):    # å¤šä¸ªæ•°æ®çš„è®¡ç®—ç»“æœ  # Calculation results of multiple data\n",
    "            action_probs = np.concatenate((action_probs, action_probs_2),axis=0)\n",
    "            value = np.concatenate((value, value_2),axis=0)\n",
    "\n",
    "                # print(\"action_probs.shape : \", np.array(action_probs).shape)\n",
    "                # print(\"value.shape : \", np.array(value).shape)\n",
    "            return action_probs, value\n",
    "        else:    # åªæœ‰1ä¸ªæ•°æ®çš„è®¡ç®—ç»“æœ  # Calculation result with only 1 data\n",
    "            return action_probs_2, value_2\n",
    "    else:\n",
    "        # æ­£å¸¸æƒ…å†µï¼Œèƒ½è¢«gpuå‡åˆ†  # Normally, it can be divided equally by gpu\n",
    "        feed_dict = {\n",
    "            self.inputs_: positions,\n",
    "            self.training: False\n",
    "        }\n",
    "        action_probs, value = self.sess.run([self.policy_head, self.value_head], feed_dict=feed_dict)\n",
    "            # print(\"np.vstack(action_probs) shape : \", np.vstack(action_probs).shape)\n",
    "            # print(\"np.vstack(value) shape : \", np.vstack(value).shape)\n",
    "        # å°†å¤šä¸ªgpuçš„è®¡ç®—ç»“æœå †å èµ·æ¥è¿”å›  # Stack the calculation results of multiple GPUs and return\n",
    "        return np.vstack(action_probs), np.vstack(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è‡ªå¯¹å¼ˆè®­ç»ƒ  Self Play Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è‡ªå¯¹å¼ˆè®­ç»ƒçš„æ€æƒ³åœ¨ä¸Šé¢åˆ†æè®ºæ–‡æ—¶å·²ç»è¯´è¿‡äº†ï¼Œç¨‹åºè‡ªå·±è·Ÿè‡ªå·±ä¸‹æ£‹ï¼Œå°†æ¯ç›˜æ£‹çš„æ•°æ®ä¿å­˜èµ·æ¥ï¼Œå½“æ•°æ®é‡è¾¾åˆ°æˆ‘ä»¬è®¾ç½®çš„å¤§å°æ—¶å°±å¼€å§‹è®­ç»ƒç¥ç»ç½‘ç»œã€‚\n",
    "\n",
    "The idea of â€‹â€‹self-play training has already been mentioned in the above analysis of the paper. The program plays chess with itself, saves the data of each game, and starts training the neural network when the amount of data reaches the size we set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(self):\n",
    "    batch_iter = 0\n",
    "    try:\n",
    "        while(True):\n",
    "            batch_iter += 1\n",
    "            play_data, episode_len = self.selfplay()    # è‡ªæˆ‘å¯¹å¼ˆï¼Œè¿”å›ä¸‹æ£‹æ•°æ®  # Self-play, return the chess data\n",
    "            print(\"batch i:{}, episode_len:{}\".format(batch_iter, episode_len))\n",
    "            extend_data = []\n",
    "            for state, mcts_prob, winner in play_data:\n",
    "                states_data = self.mcts.state_to_positions(state)\n",
    "                extend_data.append((states_data, mcts_prob, winner))    # å°†æ£‹ç›˜ç‰¹å¾å¹³é¢ã€MCTSç®—å‡ºçš„æ¦‚ç‡å‘é‡ã€èƒœç‡ä¿å­˜èµ·æ¥  # Save the chessboard feature plane, the probability vector calculated by MCTS, and the winning rate\n",
    "            self.data_buffer.extend(extend_data)\n",
    "            if len(self.data_buffer) > self.batch_size:    # ä¿å­˜çš„æ•°æ®è¾¾åˆ°æŒ‡å®šæ•°é‡æ—¶  # When the saved data reaches the specified amount\n",
    "                self.policy_update()                       # å¼€å§‹è®­ç»ƒ  # Start Training\n",
    "    except KeyboardInterrupt:\n",
    "        self.log_file.close()\n",
    "        self.policy_value_netowrk.save(self.global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### è®­ç»ƒç½‘ç»œ Training Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_update(self):\n",
    "    \"\"\"update the policy-value net\"\"\"\n",
    "    # ä»æ•°æ®ä¸­éšæœºæŠ½å–ä¸€éƒ¨åˆ†æ•°æ®  #Randomly extract a part of the data from the data\n",
    "    mini_batch = random.sample(self.data_buffer, self.batch_size)\n",
    "    #print(\"training data_buffer len : \", len(self.data_buffer))\n",
    "    state_batch = [data[0] for data in mini_batch]\n",
    "    mcts_probs_batch = [data[1] for data in mini_batch]\n",
    "    winner_batch = [data[2] for data in mini_batch]\n",
    "    # print(np.array(winner_batch).shape)\n",
    "    # print(winner_batch)\n",
    "    winner_batch = np.expand_dims(winner_batch, 1)\n",
    "    # print(winner_batch.shape)\n",
    "    # print(winner_batch)\n",
    "    start_time = time.time()\n",
    "    old_probs, old_v = self.mcts.forward(state_batch)    # å…ˆé€šè¿‡æ­£å‘ä¼ æ’­é¢„æµ‹ä¸‹ç½‘ç»œè¾“å‡ºç»“æœï¼Œç”¨äºè®¡ç®—è®­ç»ƒåçš„KLæ•£åº¦  # First predict the output of the network through forward propagation, which is used to calculate the KL divergence after training\n",
    "    for i in range(self.epochs):    # ä¸€å…±è®­ç»ƒ5æ¬¡\n",
    "        # è®­ç»ƒç½‘ç»œã€‚æ•²é»‘æ¿ï¼è¿™é‡Œçš„å­¦ä¹ ç‡éœ€è¦ç‰¹åˆ«æ³¨æ„ã€‚æˆ‘åœ¨awsä¸Šç”¨çš„æ˜¯g2.2xlargeï¼Œ24å°æ—¶åªèƒ½ä¸‹å·®ä¸å¤š200ç›˜æ£‹ï¼Œå¾ˆæ…¢ã€‚\n",
    "        # Train the network. Knock on the blackboard! The learning rate here requires special attention. I use g2.2xlarge on aws, and I can only play about 200 games in 24 hours, which is very slow.\n",
    "        # æ‰€ä»¥å­¦ä¹ ç‡æ˜¯åœ¨è¿™é‡Œæ˜¯åŠ¨æ€è°ƒæ•´çš„ã€‚å½“ç„¶æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨æŒ‡æ•°è¡°å‡å­¦ä¹ ç‡ï¼Œåœ¨ä¸Šé¢å®šä¹‰å­¦ä¹ ç‡çš„åœ°æ–¹å°±éœ€è¦ä¿®æ”¹æˆç±»ä¼¼ä¸‹é¢è¿™å¥ï¼š\n",
    "        # So the learning rate is dynamically adjusted here. Of course, you can also use the exponential decay learning rate. In the above definition of the learning rate, you need to modify it to something like the following sentence:\n",
    "        # self.learning_rate = tf.maximum(tf.train.exponential_decay(0.001, self.global_step, 1e3, 0.66), 1e-5)\n",
    "        # ç„¶åè¿™é‡Œè®­ç»ƒç½‘ç»œçš„åœ°æ–¹å­¦ä¹ ç‡å°±ä¸ç”¨ä½œä¸ºå‚æ•°ä¼ é€’äº†ï¼Œä¹Ÿå¯ä»¥åœ¨è®­ç»ƒç½‘ç»œå‡½æ•°é‡Œé¢ä¸ä½¿ç”¨ä¼ é€’çš„å­¦ä¹ ç‡å‚æ•°ã€‚\n",
    "        # # Then the learning rate where the network is trained here does not need to be passed as a parameter, and the passed learning rate parameter can also not be used in the training network function.\n",
    "        accuracy, loss, self.global_step = self.policy_value_netowrk.train_step(state_batch, mcts_probs_batch, winner_batch,\n",
    "                                                        self.learning_rate * self.lr_multiplier)    # \n",
    "        new_probs, new_v = self.mcts.forward(state_batch)    #ä½¿ç”¨è®­ç»ƒåçš„æ–°ç½‘ç»œé¢„æµ‹ç»“æœï¼Œè·Ÿä¹‹å‰çš„ç»“æœè®¡ç®—KLæ•£åº¦  #Use the new network prediction result after training, and calculate the KL divergence with the previous result\n",
    "        kl_tmp = old_probs * (np.log((old_probs + 1e-10) / (new_probs + 1e-10)))\n",
    "        # print(\"kl_tmp.shape\", kl_tmp.shape)\n",
    "        kl_lst = []\n",
    "        for line in kl_tmp:\n",
    "            # print(\"line.shape\", line.shape)\n",
    "            all_value = [x for x in line if str(x) != 'nan' and str(x)!= 'inf'] #é™¤å»infå€¼\n",
    "            kl_lst.append(np.sum(all_value))\n",
    "        kl = np.mean(kl_lst)\n",
    "        # kl = scipy.stats.entropy(old_probs, new_probs)\n",
    "        # kl = np.mean(np.sum(old_probs * (np.log(old_probs + 1e-10) - np.log(new_probs + 1e-10)), axis=1))\n",
    "\n",
    "        if kl > self.kl_targ * 4:  # early stopping if D_KL diverges badly\n",
    "            break\n",
    "    self.policy_value_netowrk.save(self.global_step)\n",
    "    print(\"train using time {} s\".format(time.time() - start_time))\n",
    "\n",
    "    # é€šè¿‡è®¡ç®—è°ƒæ•´å­¦ä¹ ç‡ä¹˜å­  # Adjust the learning rate multiplier by calculation\n",
    "    # adaptively adjust the learning rate\n",
    "    if kl > self.kl_targ * 2 and self.lr_multiplier > 0.1:\n",
    "        self.lr_multiplier /= 1.5\n",
    "    elif kl < self.kl_targ / 2 and self.lr_multiplier < 10:\n",
    "        self.lr_multiplier *= 1.5\n",
    "\n",
    "    explained_var_old = 1 - np.var(np.array(winner_batch) - old_v.flatten()) / np.var(np.array(winner_batch))\n",
    "    explained_var_new = 1 - np.var(np.array(winner_batch) - new_v.flatten()) / np.var(np.array(winner_batch))\n",
    "    print(\n",
    "        \"kl:{:.5f},lr_multiplier:{:.3f},loss:{},accuracy:{},explained_var_old:{:.3f},explained_var_new:{:.3f}\".format(\n",
    "            kl, self.lr_multiplier, loss, accuracy, explained_var_old, explained_var_new))\n",
    "    self.log_file.write(\"kl:{:.5f},lr_multiplier:{:.3f},loss:{},accuracy:{},explained_var_old:{:.3f},explained_var_new:{:.3f}\".format(\n",
    "            kl, self.lr_multiplier, loss, accuracy, explained_var_old, explained_var_new) + '\\n')\n",
    "    self.log_file.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### è‡ªæˆ‘å¯¹å¼ˆ Self Play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è‡ªæˆ‘å¯¹å¼ˆå°±æ˜¯é€šè¿‡MCTSä¸‹æ¯ä¸€æ­¥æ£‹ï¼Œç›´åˆ°åˆ†å‡ºèƒœè´Ÿï¼Œå¹¶è¿”å›ä¸‹æ£‹æ•°æ®ã€‚Self-play is to play every move through MCTS until the winner is determined and the chess data is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selfplay(self):\n",
    "    self.game_borad.reload()    # åˆå§‹åŒ–æ£‹ç›˜  # Initialize the board\n",
    "    states, mcts_probs, current_players = [], [], []\n",
    "    z = None\n",
    "    game_over = False\n",
    "    winnner = \"\"\n",
    "    start_time = time.time()\n",
    "    while(not game_over):    # ä¸‹æ£‹å¾ªç¯ï¼Œç»“æŸæ¡ä»¶æ˜¯åˆ†å‡ºèƒœè´Ÿ # Chess loop, the end condition is to decide the winner\n",
    "        action, probs, win_rate = self.get_action(self.game_borad.state, self.temperature)  # é€šè¿‡MCTSç®—å‡ºä¸‹å“ªä¸€æ­¥æ£‹  # Calculate which move to play through MCTS\n",
    "        ################################################\n",
    "        # è¿™éƒ¨åˆ†ä»£ç æ˜¯è·Ÿæˆ‘çš„è®¾è®¡æœ‰å…³çš„ã€‚å› ä¸ºåœ¨è¾“å…¥ç‰¹å¾å¹³é¢ä¸­æˆ‘æ²¡æœ‰ä½¿ç”¨é¢œè‰²ç‰¹å¾ï¼Œ\n",
    "        # æ‰€ä»¥ä¼ ç»™ç¥ç»ç½‘ç»œæ•°æ®æ—¶ï¼Œè¦æŠŠå½“å‰é€‰æ‰‹è½¬æ¢æˆçº¢è‰²ï¼ˆå…ˆæ‰‹ï¼‰ï¼Œè½¬æ¢çš„å…¶å®æ˜¯æ£‹ç›˜çš„æ£‹å­ä½ç½®\n",
    "        # è¿™æ ·ç¥ç»ç½‘ç»œé¢„æµ‹çš„å§‹ç»ˆæ˜¯çº¢è‰²å…ˆæ‰‹æ–¹å‘è¯¥å¦‚ä½•ä¸‹æ£‹\n",
    "        # This part of the code is related to my design. Because I did not use color features in the input feature plane,\n",
    "        # So when passing the neural network data, the current player must be converted to red (first move), which is actually the position of the chess pieces on the board\n",
    "        # In this way, the neural network always predicts how to play chess in the red first direction\n",
    "        state, palyer = self.mcts.try_flip(self.game_borad.state, self.game_borad.current_player, self.mcts.is_black_turn(self.game_borad.current_player))\n",
    "        states.append(state)\n",
    "        prob = np.zeros(labels_len)\n",
    "        # ç¥ç»ç½‘ç»œè¿”å›çš„æ¦‚ç‡å‘é‡ä¹Ÿéœ€è¦è½¬æ¢ï¼Œå‡å¦‚å½“å‰é€‰æ‰‹æ˜¯é»‘è‰²ï¼Œè½¬æ¢æˆçº¢è‰²åï¼Œç”±äºæ£‹ç›˜ä½ç½®çš„å˜åŒ–ï¼Œæ¦‚ç‡å‘é‡ï¼ˆèµ°å­é›†åˆï¼‰æ˜¯åŸºäºçº¢è‰²æ£‹ç›˜çš„\n",
    "        # The probability vector returned by the neural network also needs to be converted. If the current player is black, after converting to red, due to the change of the board position, the probability vector (set of moves) is based on the red board\n",
    "        # è¦æŠŠèµ°å­actionè½¬æ¢æˆé»‘è‰²é€‰æ‰‹çš„æ–¹å‘æ‰è¡Œã€‚æ˜ç™½æˆ‘çš„æ„æ€å§ï¼Ÿ\n",
    "        # The move must be converted to the direction of the black player. See what I mean?\n",
    "        if self.mcts.is_black_turn(self.game_borad.current_player):\n",
    "            for idx in range(len(probs[0][0])):\n",
    "                act = \"\".join((str(9 - int(a)) if a.isdigit() else a) for a in probs[0][0][idx])\n",
    "                prob[label2i[act]] = probs[0][1][idx]\n",
    "        else:\n",
    "            for idx in range(len(probs[0][0])):\n",
    "                prob[label2i[probs[0][0][idx]]] = probs[0][1][idx]\n",
    "        mcts_probs.append(prob)\n",
    "        ################################################\n",
    "        current_players.append(self.game_borad.current_player)\n",
    "\n",
    "        last_state = self.game_borad.state\n",
    "        self.game_borad.state = GameBoard.sim_do_action(action, self.game_borad.state) # åœ¨æ£‹ç›˜ä¸Šä¸‹ç®—å‡ºçš„è¿™æ­¥æ£‹ï¼Œå¾—åˆ°æ–°çš„æ£‹ç›˜çŠ¶æ€ # Calculate this move on the board and get the new board state\n",
    "        self.game_borad.round += 1    # æ›´æ–°å›åˆæ•°  # Update round number\n",
    "        self.game_borad.current_player = \"w\" if self.game_borad.current_player == \"b\" else \"b\" # åˆ‡æ¢å½“å‰é€‰æ‰‹  # Switch current player\n",
    "        if is_kill_move(last_state, self.game_borad.state) == 0:    # åˆšåˆšä¸‹çš„æ£‹æ˜¯å¦åƒå­äº†  # Did the chess just play have a piece?\n",
    "            self.game_borad.restrict_round += 1    # æ›´æ–°æ²¡æœ‰è¿›å±•å›åˆæ•°  # Update the number of rounds without progress\n",
    "        else:\n",
    "            self.game_borad.restrict_round = 0\n",
    "\n",
    "        if (self.game_borad.state.find('K') == -1 or self.game_borad.state.find('k') == -1):  \n",
    "            # æ¡ä»¶æ»¡è¶³è¯´æ˜å°†/å¸…è¢«åƒäº†ï¼Œæ¸¸æˆç»“æŸ  # If the conditions are met, the general/shuai was eaten, and the game is over\n",
    "            z = np.zeros(len(current_players))\n",
    "            if (self.game_borad.state.find('K') == -1):\n",
    "                winnner = \"b\"\n",
    "            if (self.game_borad.state.find('k') == -1):\n",
    "                winnner = \"w\"\n",
    "            z[np.array(current_players) == winnner] = 1.0\n",
    "            z[np.array(current_players) != winnner] = -1.0\n",
    "            game_over = True\n",
    "            print(\"Game end. Winner is player : \", winnner, \" In {} steps\".format(self.game_borad.round - 1))\n",
    "        elif self.game_borad.restrict_round >= 60:  # 60å›åˆæ²¡æœ‰è¿›å±•ï¼ˆåƒå­ï¼‰ï¼Œå¹³å±€  # 60 rounds did not progress (take the child), draw\n",
    "            z = np.zeros(len(current_players))\n",
    "            game_over = True\n",
    "            print(\"Game end. Tie in {} steps\".format(self.game_borad.round - 1))\n",
    "        # è®¤è¾“çš„éƒ¨åˆ†æ²¡æœ‰å®ç°  # The part of confession is not implemented\n",
    "        # elif(self.mcts.root.v < self.resign_threshold):\n",
    "        #     pass\n",
    "        # elif(self.mcts.root.Q < self.resign_threshold):\n",
    "        #    pass\n",
    "        if(game_over):\n",
    "            self.mcts.reload()    # æ¸¸æˆç»“æŸï¼Œé‡ç½®æ£‹ç›˜  # Game over, reset the board\n",
    "    print(\"Using time {} s\".format(time.time() - start_time))\n",
    "    return zip(states, mcts_probs, z), len(z)    # è¿”å›ä¸‹æ£‹æ•°æ®  # Return the chess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCTSå®ç°      \n",
    "MCTS implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å…³é”®çš„ä»£ç æ¥äº†ï¼Œå‡½æ•°é€šè¿‡MCTSè¿›è¡Œè‹¥å¹²æ¬¡æ¨¡æ‹Ÿï¼ˆè®ºæ–‡æ˜¯1600æ¬¡ï¼Œæˆ‘ç”¨äº†1200æ¬¡ï¼‰ï¼Œç„¶åæ ¹æ®å­èŠ‚ç‚¹çš„è®¿é—®é‡å†³å®šè¦ä¸‹å“ªæ­¥æ£‹ã€‚\n",
    "\n",
    "The key code is here. The function performs several simulations through MCTS (1600 times for the paper, I used 1200 times), and then decides which move to play based on the number of visits to the child nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@profile\n",
    "def get_action(self, state, temperature = 1e-3):\n",
    "    # MCTSä¸»å‡½æ•°ï¼Œæ¨¡æ‹Ÿä¸‹æ£‹\n",
    "    # MCTS main function, simulated chess\n",
    "    self.mcts.main(state, self.game_borad.current_player, self.game_borad.restrict_round, self.playout_counts)\n",
    "    # å–å¾—å½“å‰å±€é¢ä¸‹æ‰€æœ‰å­èŠ‚ç‚¹çš„åˆæ³•èµ°å­å’Œç›¸åº”çš„è®¿é—®é‡ã€‚\n",
    "    # è¿™ä¸ªæ‰€æœ‰å­èŠ‚ç‚¹å¯èƒ½å¹¶ä¸ä¼šè¦†ç›–æ‰€æœ‰åˆæ³•çš„èµ°å­ï¼Œè¿™ä¸ªæ˜¯ç”±æ ‘æœç´¢çš„è´¨é‡å†³å®šçš„ï¼ŒåŠ å¤§æ¨¡æ‹Ÿæ¬¡æ•°ä¼šæœç´¢æ›´å¤šä¸åŒçš„èµ°æ³•ï¼Œ\n",
    "    # å°±æ˜¯åŠ å¤§æ€è€ƒçš„æ·±åº¦ï¼Œè€ƒè™‘æ›´å¤šçš„å±€é¢ï¼Œé¿å…å‡ºç°æœ‰äº›ç‰¹åˆ«é‡è¦çš„æ£‹æ­¥å´æ²¡æœ‰è€ƒè™‘åˆ°çš„æƒ…å†µã€‚\n",
    "    # Get the legal moves and corresponding visits of all child nodes in the current situation.\n",
    "    # All child nodes may not cover all legal moves. This is determined by the quality of the tree search. Increasing the number of simulations will search for more different moves.\n",
    "    # Is to increase the depth of thinking, consider more situations, and avoid situations where some particularly important moves are not considered.\n",
    "    actions_visits = [(act, nod.N) for act, nod in self.mcts.root.child.items()]\n",
    "    actions, visits = zip(*actions_visits)\n",
    "\n",
    "    probs = softmax(1.0 / temperature * np.log(visits))    #+ 1e-10\n",
    "    move_probs = []\n",
    "    move_probs.append([actions, probs])\n",
    "\n",
    "    if(self.exploration):\n",
    "        # è®­ç»ƒæ—¶ï¼Œå¯ä»¥é€šè¿‡åŠ å…¥å™ªå£°æ¥æ¢ç´¢æ›´å¤šå¯èƒ½æ€§çš„èµ°å­\n",
    "        # When training, you can explore more possibilities of walking by adding noise\n",
    "        act = np.random.choice(actions, p=0.75 * probs + 0.25*np.random.dirichlet(0.3*np.ones(len(probs))))\n",
    "    else:\n",
    "        act = np.random.choice(actions, p=probs)     # é€šè¿‡èŠ‚ç‚¹è®¿é—®é‡çš„softmaxé€‰æ‹©æœ€å¤§å¯èƒ½æ€§çš„èµ°å­  # Select the most likely mover through the softmax of node visits\n",
    "\n",
    "    win_rate = self.mcts.Q(act) # å°†èŠ‚ç‚¹çš„Qå€¼å½“åšèƒœç‡  # Take the Q value of the node as the winning rate\n",
    "    self.mcts.update_tree(act) # æ›´æ–°æœç´¢æ ‘ï¼Œå°†ç®—å‡ºçš„è¿™æ­¥æ£‹çš„å±€é¢ä½œä¸ºæ ‘çš„æ ¹èŠ‚ç‚¹  # Update the search tree and use the calculated position as the root node of the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¥çœ‹çœ‹MCTSçš„ç±»å®šä¹‰ï¼š Take a look at the MCTS class definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, defaultdict, namedtuple\n",
    "QueueItem = namedtuple(\"QueueItem\", \"feature future\")\n",
    "c_PUCT = 5\n",
    "virtual_loss = 3\n",
    "cut_off_depth = 30\n",
    "\n",
    "class MCTS_tree(object):\n",
    "    def __init__(self, in_state, in_forward, search_threads):    # å‚æ•°search_threadsæˆ‘é»˜è®¤ä½¿ç”¨16ä¸ªæœç´¢çº¿ç¨‹  # Parameter search_threads I use 16 search threads by default\n",
    "        self.noise_eps = 0.25\n",
    "        self.dirichlet_alpha = 0.3    #0.03\n",
    "        # æ ¹èŠ‚ç‚¹çš„å…ˆéªŒæ¦‚ç‡åŠ å…¥äº†å™ªå£°  # The prior probability of the root node adds noise\n",
    "        self.p_ = (1 - self.noise_eps) * 1 + self.noise_eps * np.random.dirichlet([self.dirichlet_alpha])\n",
    "        # å®šä¹‰æ ¹èŠ‚ç‚¹ï¼Œä¼ å…¥æ¦‚ç‡å’Œå±€é¢ï¼ˆæ£‹å­ä½ç½®ï¼‰  # Define the root node, incoming probability and position (position of chess pieces)\n",
    "        self.root = leaf_node(None, self.p_, in_state)\n",
    "        self.c_puct = 5    #1.5\n",
    "        # ä¿å­˜å‰å‘ä¼ æ’­ï¼ˆé¢„æµ‹ï¼‰å‡½æ•°  # Save the forward propagation (prediction) function\n",
    "        self.forward = in_forward\n",
    "        self.node_lock = defaultdict(Lock)\n",
    "        # è™šæ‹ŸæŸå¤±  #Virtual Loss\n",
    "        self.virtual_loss = 3\n",
    "        # ç”¨æ¥ä¿å­˜æ­£åœ¨æ‰©å±•çš„èŠ‚ç‚¹  # Used to save the node being expanded\n",
    "        self.now_expanding = set()\n",
    "        # ä¿å­˜æ‰©å±•è¿‡çš„èŠ‚ç‚¹   # Save the expanded node\n",
    "        self.expanded = set()\n",
    "        self.cut_off_depth = 30\n",
    "        # self.QueueItem = namedtuple(\"QueueItem\", \"feature future\")\n",
    "        self.sem = asyncio.Semaphore(search_threads)\n",
    "        # ä¿å­˜æœç´¢çº¿ç¨‹çš„é˜Ÿåˆ—  # Save the queue of search threads\n",
    "        self.queue = Queue(search_threads)\n",
    "        self.loop = asyncio.get_event_loop()\n",
    "        self.running_simulation_num = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¶å­èŠ‚ç‚¹çš„ç±»å®šä¹‰ï¼š The class definition of the leaf node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class leaf_node(object):\n",
    "    # å®šä¹‰èŠ‚ç‚¹æ—¶ï¼Œä¼ å…¥çˆ¶èŠ‚ç‚¹ï¼Œæ¦‚ç‡å’Œæ£‹ç›˜çŠ¶æ€ï¼ˆæ£‹å­ä½ç½®ï¼‰  # When defining a node, pass in the parent node, probability and board state (position of the chess pieces)\n",
    "    def __init__(self, in_parent, in_prior_p, in_state):\n",
    "        self.P = in_prior_p    # ä¿å­˜æ¦‚ç‡ï¼Œå…¶ä»–å€¼é»˜è®¤æ˜¯0  # Save the probability, other values â€‹â€‹are 0 by default\n",
    "        self.Q = 0\n",
    "        self.N = 0\n",
    "        self.v = 0\n",
    "        self.U = 0\n",
    "        self.W = 0\n",
    "        self.parent = in_parent  # ä¿å­˜çˆ¶èŠ‚ç‚¹  # Save the parent node\n",
    "        self.child = {}    # å­èŠ‚ç‚¹é»˜è®¤æ˜¯ç©º  # The child node is empty by default\n",
    "        self.state = in_state    # ä¿å­˜æ£‹ç›˜çŠ¶æ€  # Save the board state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MCTSä¸»å‡½æ•°ï¼Œæ¨¡æ‹Ÿä¸‹æ£‹:\n",
    "\n",
    "MCTS main function, simulating chess:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_expanded(self, key) -> bool:\n",
    "    \"\"\"Check expanded status\"\"\"\n",
    "    return key in self.expanded\n",
    "    \n",
    "#@profile\n",
    "def main(self, state, current_player, restrict_round, playouts):\n",
    "    node = self.root\n",
    "    # å…ˆé€šè¿‡ç¥ç»ç½‘ç»œæ‰©å±•æ ¹èŠ‚ç‚¹  # First expand the root node through the neural network\n",
    "    if not self.is_expanded(node):    # and node.is_leaf()    # node.state\n",
    "        # print('Expadning Root Node...')\n",
    "        positions = self.generate_inputs(node.state, current_player)\n",
    "        positions = np.expand_dims(positions, 0)\n",
    "        action_probs, value = self.forward(positions)    # é€šè¿‡ç¥ç»ç½‘ç»œé¢„æµ‹èµ°å­æ¦‚ç‡  # Predict the probability of walking through a neural network\n",
    "        if self.is_black_turn(current_player):    # åˆ¤æ–­èµ°å­æ¦‚ç‡æ˜¯å¦éœ€è¦æ ¹æ®å…ˆæ‰‹/åæ‰‹è¿›è¡Œè½¬æ¢  # Determine whether the probability of a move needs to be converted according to the first hand/back hand\n",
    "            action_probs = cchess_main.flip_policy(action_probs)\n",
    "        # å–å¾—å½“å‰å±€é¢æ‰€æœ‰åˆæ³•çš„èµ°å­ï¼Œæœ‰å…³ä¸­å›½è±¡æ£‹çš„ç®—æ³•å°±ä¸åœ¨è¿™é‡Œè®¨è®ºäº†ï¼Œæ„Ÿå…´è¶£å¯ä»¥æŸ¥çœ‹æºä»£ç   # Get all the legal moves in the current situation. The algorithm of Chinese chess is not discussed here. If you are interested, you can view the source code\n",
    "        moves = GameBoard.get_legal_moves(node.state, current_player)    \n",
    "        # print(\"current_player : \", current_player)\n",
    "        # print(moves)\n",
    "        node.expand(moves, action_probs)    # æ‰©å±•èŠ‚ç‚¹  # Expansion node\n",
    "        self.expanded.add(node)    # å°†å½“å‰èŠ‚ç‚¹åŠ å…¥åˆ°å·²æ‰©å±•èŠ‚ç‚¹é›†åˆä¸­  # Add the current node to the expanded node set\n",
    "\n",
    "    coroutine_list = []\n",
    "    for _ in range(playouts):    # æ¨¡æ‹Ÿ1200æ¬¡ï¼Œå¼‚æ­¥çš„æ–¹å¼æ‰§è¡Œï¼Œä¸€å…±ä½¿ç”¨äº†16ä¸ªçº¿ç¨‹  # Simulate 1200 times, execute asynchronously, using a total of 16 threads\n",
    "        coroutine_list.append(self.tree_search(node, current_player, restrict_round))\n",
    "    coroutine_list.append(self.prediction_worker())\n",
    "    self.loop.run_until_complete(asyncio.gather(*coroutine_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def tree_search(self, node, current_player, restrict_round) -> float:\n",
    "    \"\"\"Independent MCTS, stands for one simulation\"\"\"\n",
    "    self.running_simulation_num += 1\n",
    "\n",
    "    # reduce parallel search number\n",
    "    with await self.sem:    # å¼‚æ­¥æ‰§è¡Œæ ‘æœç´¢ï¼Œ # Asynchronous execution tree search, a total of 16 threads\n",
    "        value = await self.start_tree_search(node, current_player, restrict_round)\n",
    "        self.running_simulation_num -= 1\n",
    "\n",
    "        return value\n",
    "\n",
    "# ***æ ‘æœç´¢å‡½æ•°***  # ***Tree search function***\n",
    "async def start_tree_search(self, node, current_player, restrict_round)->float:\n",
    "    \"\"\"Monte Carlo Tree search Select,Expand,Evauate,Backup\"\"\"\n",
    "    now_expanding = self.now_expanding\n",
    "\n",
    "    # å¦‚æœå½“å‰èŠ‚ç‚¹æ­£åœ¨è¢«æ‰©å±•ï¼Œå°±å°ç¡ä¸€ä¼š  # If the current node is being expanded, take a nap\n",
    "    while node in now_expanding:\n",
    "        await asyncio.sleep(1e-4)\n",
    "\n",
    "    if not self.is_expanded(node):    # å¦‚æœèŠ‚ç‚¹æ²¡æœ‰è¢«æ‰©å±•è¿‡ï¼Œè¦æ‰©å±•è¿™ä¸ªèŠ‚ç‚¹  # If the node has not been expanded, expand this node\n",
    "        \"\"\"is leaf node try evaluate and expand\"\"\"\n",
    "        # add leaf node to expanding list\n",
    "        self.now_expanding.add(node)  # åŠ å…¥åˆ°æ­£åœ¨æ‰©å±•é›†åˆä¸­  # Add to the expanding collection\n",
    "\n",
    "        positions = self.generate_inputs(node.state, current_player)\n",
    "\n",
    "        # è¿™é‡Œæœ‰ä¸ªtrickï¼Œå°±æ˜¯å¹¶ä¸æ˜¯ä¸€ä¸ªèŠ‚ç‚¹ä¸€ä¸ªèŠ‚ç‚¹çš„ä½¿ç”¨ç¥ç»ç½‘ç»œé¢„æµ‹ç»“æœï¼Œè¿™æ ·æ•ˆç‡å¤ªä½\n",
    "        # There is a trick here, that is, it is not a node-by-node using neural network to predict the results, so the efficiency is too low\n",
    "        # è€Œæ˜¯æ”¾åˆ°é˜Ÿåˆ—ä¸­ï¼Œé€šè¿‡prediction_workerå‡½æ•°ç»Ÿä¸€ç®¡ç†é˜Ÿåˆ—ï¼Œå°†é˜Ÿåˆ—ä¸­çš„ä¸€ç»„ï¼ˆ16ä¸ªï¼‰è¾“å…¥ä¼ ç»™ç¥ç»ç½‘ç»œï¼Œå¾—åˆ°é¢„æµ‹ç»“æœ\n",
    "        # Instead, put it in the queue, manage the queue uniformly through the prediction_worker function, and pass a set of (16) inputs in the queue to the neural network to get the prediction result\n",
    "        # è¿™ä¸€åˆ‡éƒ½æ˜¯å¼‚æ­¥çš„\n",
    "        # All this is asynchronous\n",
    "        # push extracted dihedral features of leaf node to the evaluation queue\n",
    "        future = await self.push_queue(positions)  # type: Future\n",
    "        await future\n",
    "        action_probs, value = future.result()\n",
    "\n",
    "        if self.is_black_turn(current_player):    # æ ¹æ®å½“å‰æ£‹æ‰‹çš„é¢œè‰²å†³å®šæ˜¯å¦å¯¹èµ°å­æ¦‚ç‡ç¿»è½¬  # Determine whether to flip the move probability according to the current player's color\n",
    "            action_probs = cchess_main.flip_policy(action_probs)\n",
    "\n",
    "        moves = GameBoard.get_legal_moves(node.state, current_player)\n",
    "        # print(\"current_player : \", current_player)\n",
    "        # print(moves)\n",
    "        node.expand(moves, action_probs)    # Expandæ“ä½œï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œé¢„æµ‹çš„ç»“æœæ‰©å±•å½“å‰èŠ‚ç‚¹  # Expand operation, use the results predicted by the neural network to expand the current node\n",
    "        self.expanded.add(node)  # \n",
    "\n",
    "        # remove leaf node from expanding list\n",
    "        self.now_expanding.remove(node)\n",
    "\n",
    "        # must invert, because alternative layer has opposite objective\n",
    "        return value[0] * -1    # è¿”å›ç¥ç»ç½‘ç»œé¢„æµ‹çš„èƒœç‡ï¼Œä¸€å®šè¦å–è´Ÿï¼Œç†ç”±åœ¨è®ºæ–‡åˆ†ææ—¶å·²ç»è¯´è¿‡äº†  # Return the winning rate predicted by the neural network, and it must be negative. The reason has already been said in the analysis of the paper\n",
    "    else:    # å¦‚æœèŠ‚ç‚¹è¢«æ‰©å±•è¿‡ï¼Œæ‰§è¡ŒSelect  # If the node has been expanded, execute Select\n",
    "        \"\"\"node has already expanded. Enter select phase.\"\"\"\n",
    "        # select child node with maximum action scroe\n",
    "        last_state = node.state\n",
    "\n",
    "        action, node = node.select_new(c_PUCT)  # Selectæ“ä½œï¼Œæ ¹æ®Q+Uæœ€å¤§é€‰æ‹©èŠ‚ç‚¹  # Select operation, select the node according to the maximum Q+U\n",
    "        current_player = \"w\" if current_player == \"b\" else \"b\"\n",
    "        if is_kill_move(last_state, node.state) == 0:\n",
    "            restrict_round += 1\n",
    "        else:\n",
    "            restrict_round = 0\n",
    "        last_state = node.state\n",
    "\n",
    "        # ä¸ºé€‰æ‹©çš„èŠ‚ç‚¹æ·»åŠ è™šæ‹ŸæŸå¤±ï¼Œé˜²æ­¢å…¶ä»–çº¿ç¨‹ç»§ç»­æ¢ç´¢è¿™ä¸ªèŠ‚ç‚¹ï¼Œå¢åŠ æ¢ç´¢å¤šæ ·æ€§\n",
    "        # Add a virtual loss to the selected node to prevent other threads from continuing to explore this node and increase the diversity of exploration\n",
    "        # add virtual loss\n",
    "        node.N += virtual_loss\n",
    "        node.W += -virtual_loss\n",
    "\n",
    "        # evolve game board status\n",
    "        # åˆ¤æ–­è¿™ä¸ªèŠ‚ç‚¹çŠ¶æ€ä¸‹ï¼Œæ˜¯å¦åˆ†å‡ºèƒœè´Ÿ\n",
    "        # Judge whether this node state is a winner\n",
    "        if (node.state.find('K') == -1 or node.state.find('k') == -1):\n",
    "            # åˆ†å‡ºèƒœè´Ÿäº†ï¼Œè®¾ç½®èƒœç‡1æˆ–è€…0  # The winner is divided, set the winning rate to 1 or 0\n",
    "            if (node.state.find('K') == -1):\n",
    "                value = 1.0 if current_player == \"b\" else -1.0\n",
    "            if (node.state.find('k') == -1):\n",
    "                value = -1.0 if current_player == \"b\" else 1.0\n",
    "            # ä¸€å®šè¦ç¬¦å·å–å  # The sign must be reversed\n",
    "            value = value * -1\n",
    "        elif restrict_round >= 60:    # 60å›åˆæ— è¿›å±•ï¼ˆåƒå­ï¼‰ï¼Œå¹³å±€ # 60 rounds no progress (take a child), draw\n",
    "            value = 0.0\n",
    "        else:\n",
    "            # æ²¡æœ‰åˆ†å‡ºèƒœè´Ÿï¼Œåœ¨å½“å‰èŠ‚ç‚¹å±€é¢ä¸‹ç»§ç»­æ ‘æœç´¢  # There is no winner or loser, continue the tree search under the current node position\n",
    "            value = await self.start_tree_search(node, current_player, restrict_round)  # next move\n",
    "\n",
    "        # å½“å‰èŠ‚ç‚¹æœç´¢å®Œæ¯•ï¼Œå»æ‰è™šæ‹ŸæŸå¤±ï¼Œæ¢å¤èŠ‚ç‚¹çŠ¶æ€  # The current node search is completed, remove the virtual loss and restore the node state\n",
    "        node.N += -virtual_loss\n",
    "        node.W += virtual_loss\n",
    "\n",
    "        # on returning search path\n",
    "        # update: N, W, Q, U\n",
    "        node.back_up_value(value)    # æ‰§è¡ŒèŠ‚ç‚¹çš„Backupæ“ä½œï¼Œæ›´æ–°èŠ‚ç‚¹çš„å„ç±»æ•°å€¼  # Execute the backup operation of the node and update the various values â€‹â€‹of the node\n",
    "\n",
    "        # must invert\n",
    "        return value * -1    # ä¸€å®šè¦ç¬¦å·å–å  # The sign must be reversed\n",
    "\n",
    "# ç®¡ç†é˜Ÿåˆ—æ•°æ®ï¼Œä¸€æ—¦é˜Ÿåˆ—ä¸­æœ‰æ•°æ®ï¼Œå°±ç»Ÿä¸€ä¼ ç»™ç¥ç»ç½‘ç»œï¼Œè·å¾—é¢„æµ‹ç»“æœ\n",
    "# Manage queue data, once there is data in the queue, it will be uniformly transmitted to the neural network to obtain the prediction result\n",
    "async def prediction_worker(self):\n",
    "    \"\"\"For better performance, queueing prediction requests and predict together in this worker.\n",
    "    speed up about 45sec -> 15sec for example.\n",
    "    \"\"\"\n",
    "    q = self.queue\n",
    "    margin = 10  # avoid finishing before other searches starting.\n",
    "    while self.running_simulation_num > 0 or margin > 0:\n",
    "        if q.empty():\n",
    "            if margin > 0:\n",
    "                margin -= 1\n",
    "            await asyncio.sleep(1e-3)\n",
    "            continue\n",
    "        item_list = [q.get_nowait() for _ in range(q.qsize())]  # type: list[QueueItem]\n",
    "\n",
    "        features = np.asarray([item.feature for item in item_list])    # \n",
    "\n",
    "        action_probs, value = self.forward(features)\n",
    "        for p, v, item in zip(action_probs, value, item_list):\n",
    "            item.future.set_result((p, v))\n",
    "\n",
    "async def push_queue(self, features):\n",
    "    future = self.loop.create_future()\n",
    "    item = QueueItem(features, future)\n",
    "    await self.queue.put(item)\n",
    "    return future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ€åçœ‹çœ‹å¶å­èŠ‚ç‚¹çš„Selectã€Expandå’ŒBackupçš„å®ç°ã€‚\n",
    "Finally, look at the implementation of Select, Expand and Backup of leaf nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selectï¼Œé€‰æ‹©Q+Uæœ€å¤§çš„èŠ‚ç‚¹  # Select, select the node with the largest Q+U\n",
    "def select_new(self, c_puct):\n",
    "    return max(self.child.items(), key=lambda node: node[1].get_Q_plus_U_new(c_puct))\n",
    "\n",
    "# è¿”å›èŠ‚ç‚¹çš„Q+U  # Return the Q+U of the node\n",
    "def get_Q_plus_U_new(self, c_puct):\n",
    "    \"\"\"Calculate and return the value for this node: a combination of leaf evaluations, Q, and\n",
    "    this node's prior adjusted for its visit count, u\n",
    "    c_puct -- a number in (0, inf) controlling the relative impact of values, Q, and\n",
    "        prior probability, P, on this node's score.\n",
    "    \"\"\"\n",
    "    U = c_puct * self.P * np.sqrt(self.parent.N) / ( 1 + self.N)\n",
    "    return self.Q + U\n",
    "\n",
    "# å‚æ•°æ˜¯æ‰€æœ‰åˆæ³•èµ°å­movesï¼Œå’Œç¥ç»ç½‘ç»œé¢„æµ‹çš„æ¦‚ç‡å‘é‡action_probs\n",
    "# The parameter is all legal moves, and the probability vector predicted by the neural network action_probs\n",
    "#@profile\n",
    "def expand(self, moves, action_probs):\n",
    "    tot_p = 1e-8\n",
    "    action_probs = action_probs.flatten()   \n",
    "    \n",
    "    for action in moves:\n",
    "        # æ¨¡æ‹Ÿæ‰§è¡Œæ¯ä¸€ä¸ªåˆæ³•èµ°å­ï¼Œå¾—åˆ°ç›¸åº”çš„å±€é¢ï¼ˆæ£‹å­ä½ç½®ï¼‰ # Simulate each legal move and get the corresponding position (position of the chess piece)\n",
    "        in_state = GameBoard.sim_do_action(action, self.state)\n",
    "        # ä»æ¦‚ç‡å‘é‡ä¸­å¾—åˆ°å½“å‰èµ°å­å¯¹åº”çš„æ¦‚ç‡  # Get the probability corresponding to the current move from the probability vector\n",
    "        mov_p = action_probs[label2i[action]]\n",
    "        # åˆ›å»ºæ–°èŠ‚ç‚¹ï¼Œä¼ å…¥çˆ¶èŠ‚ç‚¹ï¼ˆå› ä¸ºæ˜¯æ‰©å±•å½“å‰èŠ‚ç‚¹ï¼Œæ‰€ä»¥å½“å‰èŠ‚ç‚¹æ˜¯æ–°èŠ‚ç‚¹çš„çˆ¶èŠ‚ç‚¹ï¼‰ã€æ¦‚ç‡ã€æ£‹ç›˜çŠ¶æ€  # Create a new node, pass in the parent node (because the current node is expanded, so the current node is the parent node of the new node), probability, and board state\n",
    "        new_node = leaf_node(self, mov_p, in_state)\n",
    "        self.child[action] = new_node    # å°†æ–°èŠ‚ç‚¹æ·»åŠ åˆ°å½“å‰èŠ‚ç‚¹çš„å­èŠ‚ç‚¹é›†åˆä¸­  # Add the new node to the set of child nodes of the current node\n",
    "        tot_p += mov_p    \n",
    "    \n",
    "    for a, n in self.child.items():\n",
    "        n.P /= tot_p\n",
    "\n",
    "# æ›´æ–°èŠ‚ç‚¹çš„å„é¡¹å‚æ•°  # Update the parameters of the node\n",
    "def back_up_value(self, value):\n",
    "    self.N += 1    # è®¡æ•°åŠ ä¸€  # Count plus one\n",
    "    self.W += value    # æ›´æ–°æ€»è¡ŒåŠ¨ä»·å€¼  # Update total action value\n",
    "    self.v = value    \n",
    "    self.Q = self.W / self.N  # æ›´æ–°å¹³å‡è¡ŒåŠ¨ä»·å€¼  # Update average action value\n",
    "    self.U = c_PUCT * self.P * np.sqrt(self.parent.N) / ( 1 + self.N)  # æ›´æ–°U  # Update U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»¥ä¸Šï¼Œå°±æ˜¯è‡ªå¯¹å¼ˆè®­ç»ƒç¥ç»ç½‘ç»œçš„å…¨éƒ¨å†…å®¹äº†ï¼Œå…³äºä¸­å›½è±¡æ£‹çš„å®ç°éƒ¨åˆ†è¯·çœ‹é¡¹ç›®ä»£ç ã€‚\n",
    "The above is the whole content of the self-play training neural network. For the realization of Chinese chess, please see the project code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æœ€å Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘æ¥è¯´è¯´è®­ç»ƒæƒ…å†µï¼Œå› ä¸ºæ˜¯ä»ç™½æ¿ä¸€å—å¼€å§‹è®­ç»ƒï¼Œåˆšå¼€å§‹éƒ½æ˜¯ä¹±ä¸‹ï¼Œä»ä¹±ä¸‹çš„ç»éªŒå½“ä¸­å­¦ä¼šä¸‹æ£‹æ˜¯éœ€è¦å¤§é‡å¯¹å¼ˆæ‰è¡Œçš„ã€‚è§£çš„ç©ºé—´æ˜¯å¾ˆç¨€ç–çš„ï¼Œç›¸å½“äº100ä¸ªæ•°æ®ï¼Œæœ‰99ä¸ªæ˜¯è´Ÿä¾‹ï¼Œåªæœ‰1ä¸ªæ­£ä¾‹ã€‚è®ºæ–‡ä¸­è®­ç»ƒäº†700Kæ¬¡çš„mini-batchï¼Œå›½é™…è±¡æ£‹å¼€æºé¡¹ç›®[`chess-alpha-zero`](https://github.com/Zeta36/chess-alpha-zero)ä¹Ÿè®­ç»ƒäº†10Kæ¬¡ã€‚æˆ‘å‘¢ï¼Œè®­ç»ƒä¸åˆ°4Kæ¬¡ï¼Œæ¨¡å‹åˆšåˆšå­¦ä¼šç”¨è±¡å’Œå£«é˜²å®ˆï¼Œæ€»ä¹‹ä»ç„¶ä¸‹æ£‹å¾ˆçƒ‚ã€‚å¦‚æœæ‚¨æœ‰æ¡ä»¶å¯ä»¥å†å¤šè®­ç»ƒè¯•è¯•ï¼Œæˆ‘è‡ªä»æ”¶åˆ°ä¿¡ç”¨å¡æ‰£æ¬¾400ç¾å…ƒé€šçŸ¥ä»¥åå°±æŠŠawsä¸‹çº¿äº†ï¼šD è´«ç©·é™åˆ¶äº†æˆ‘çš„æƒ³è±¡åŠ›O(âˆ©_âˆ©)O\n",
    "\n",
    "Let me talk about the training situation, because I started training with the whiteboard. At the beginning, it was chaotic. Learning to play chess from chaotic experience requires a lot of games. The solution space is very sparse, equivalent to 100 data, 99 are negative examples, and only 1 positive example. In the paper, mini-batch was trained 700K times, and [`chess-alpha-zero`](https://github.com/Zeta36/chess-alpha-zero), an open source chess project, was also trained 10K times. As for me, I have trained less than 4K times. The model has just learned to defend with elephants and warriors. In short, I still play chess badly. If you have the conditions, you can try more training. I have taken aws offline since I received the 400 USD credit card charge notification: D Poverty limits my imagination O(âˆ©_âˆ©)O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å‚è€ƒèµ„æ–™\n",
    "\n",
    "Reference materialÂ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - [`æ·±å…¥æµ…å‡ºçœ‹æ‡‚AlphaGoå…ƒ`](https://charlesliuyx.github.io/2017/10/18/æ·±å…¥æµ…å‡ºçœ‹æ‡‚AlphaGoå…ƒ/)\n",
    " - [`æ·±å…¥æµ…å‡ºçœ‹æ‡‚AlphaGoå¦‚ä½•ä¸‹æ£‹`](https://charlesliuyx.github.io/2017/05/27/AlphaGoè¿è¡ŒåŸç†è§£æ/)\n",
    " - å›´æ£‹å¼€æºé¡¹ç›®[`AlphaGOZero-python-tensorflow`](https://github.com/yhyu13/AlphaGOZero-python-tensorflow)\n",
    " - [`TensorFlowå¤šGPUå¹¶è¡Œè®¡ç®—å®ä¾‹---MNIST`](https://gitee.com/liyang619/mnist_multi_gpu_batching_train/blob/master/mnist_multi_gpu_batching_train.py)\n",
    " - å›½é™…è±¡æ£‹å¼€æºé¡¹ç›®[`chess-alpha-zero`](https://github.com/Zeta36/chess-alpha-zero)\n",
    " - [`FENæ–‡ä»¶æ ¼å¼`](http://www.xqbase.com/protocol/cchess_fen.htm)\n",
    " - [`ç€æ³•è¡¨ç¤º`](http://www.xqbase.com/protocol/cchess_move.htm)\n",
    " - [`ä¸­å›½è±¡æ£‹é€šç”¨å¼•æ“åè®®ã€€ç‰ˆæœ¬ï¼š3.0`](http://www.xqbase.com/protocol/cchess_ucci.htm)\n",
    " - äº”å­æ£‹å¼€æºé¡¹ç›®[`AlphaZero_Gomoku`](https://github.com/junxiaosong/AlphaZero_Gomoku)\n",
    " - é»‘ç™½æ£‹å¼€æºé¡¹ç›®[`reversi-alpha-zero`](https://github.com/mokemokechicken/reversi-alpha-zero)\n",
    " - ä¸­å›½è±¡æ£‹å¼€æºé¡¹ç›®[`IntelliChess`](https://github.com/lifei96/IntelliChess)\n",
    " - ä¸­å›½è±¡æ£‹UIé¡¹ç›®[`ChineseChess`](https://github.com/Linzertorte/ChineseChess)\n",
    " \n",
    " \n",
    " Translation (English)\n",
    " \n",
    " - [`In-depth understanding of AlphaGo yuan`](https://charlesliuyx.github.io/2017/10/18/æ·±å…¥æµ…å‡ºçœ‹æ‡‚AlphaGoå…ƒ/)\n",
    " - [`Understand how AlphaGo plays chess`](https://charlesliuyx.github.io/2017/05/27/AlphaGoè¿è¡ŒåŸç†è§£æ/)\n",
    " - Go open source project [`AlphaGOZero-python-tensorflow`](https://github.com/yhyu13/AlphaGOZero-python-tensorflow)\n",
    " - [`TensorFlow Multi-GPU Parallel Computing Example---MNIST`](https://gitee.com/liyang619/mnist_multi_gpu_batching_train/blob/master/mnist_multi_gpu_batching_train.py)\n",
    " - Chess open source project [`chess-alpha-zero`](https://github.com/Zeta36/chess-alpha-zero)\n",
    " - [`FEN file format`](http://www.xqbase.com/protocol/cchess_fen.htm)\n",
    " - [`Movement representation`](http://www.xqbase.com/protocol/cchess_move.htm)\n",
    " - [`Chinese Chess General Engine Protocolã€€Version: 3.0`](http://www.xqbase.com/protocol/cchess_ucci.htm)\n",
    " - Gobang open source project [`AlphaZero_Gomoku`](https://github.com/junxiaosong/AlphaZero_Gomoku)\n",
    " - Othello open source project [`reversi-alpha-zero`](https://github.com/mokemokechicken/reversi-alpha-zero)\n",
    " - Chinese Chess Open Source Project [`IntelliChess`](https://github.com/lifei96/IntelliChess)\n",
    " - Chinese Chess UI project [`ChineseChess`](https://github.com/Linzertorte/ChineseChess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä»Šå¤©çš„åˆ†äº«å°±åˆ°è¿™é‡Œï¼Œè¯·å¤šæŒ‡æ•™~\n",
    "Todayâ€™s sharing is here, please advise~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
